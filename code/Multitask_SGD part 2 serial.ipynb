{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.corpora.dictionary import Dictionary\n",
      "import scipy.sparse\n",
      "\n",
      "from __future__ import division\n",
      "import __builtin__\n",
      "\n",
      "import copy\n",
      "import subprocess, shlex, os\n",
      "import time\n",
      "import pickle\n",
      "from collections import defaultdict, OrderedDict\n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"../working/N.pickle\", 'r') as picklefile:\n",
      "    N = pickle.load(picklefile)\n",
      "print \"N:\", N\n",
      "    \n",
      "with open(\"../working/bodies_lst.pickle\", 'r') as picklefile:\n",
      "    bodies_lst = pickle.load(picklefile)\n",
      "\n",
      "# with open(\"../working/titles_lst.pickle\", 'r') as picklefile:\n",
      "#     titles_lst = pickle.load(picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 4095395\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Build a tag <-> list of all indices of rows that contain the tag\n",
      "# with open(\"../working/tags_lst.pickle\", 'r') as picklefile:\n",
      "#     tags_lst = pickle.load(picklefile)\n",
      "\n",
      "# tag_rows_index = defaultdict(list)\n",
      "# for i, tags_line in enumerate(tags_lst):\n",
      "#     for tag in tags_line.split():\n",
      "#         tag_rows_index[tag].append(i)\n",
      "        \n",
      "# # Sort tag_rows_index by occurrence\n",
      "# sorted_items = sorted(tag_rows_index.iteritems(), key=lambda x: len(x[1]), reverse=True)\n",
      "# tag_rows_index = OrderedDict(sorted_items)\n",
      "\n",
      "# with open(\"../working/tag_rows_index.pickle\", 'w') as picklefile:\n",
      "#     pickle.dump(tag_rows_index, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"../working/tag_rows_index.pickle\", 'r') as picklefile:\n",
      "    tag_rows_index = pickle.load(picklefile)\n",
      "\n",
      "tags_index = tag_rows_index.iterkeys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up the sampling probabilities for each tag\n",
      "tag_counts = {k:len(v) for (k,v) in tag_rows_index.iteritems()}\n",
      "total_count = __builtin__.sum(tag_counts.itervalues())\n",
      "tag_probs = {k:v/total_count for (k,v) in tag_counts.iteritems()}\n",
      "labels, probs = tag_probs.keys(), tag_probs.values()\n",
      "del tag_probs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"len(tag_rows_index):\", len(tag_rows_index)\n",
      "print \"len(tag_counts):\", len(tag_counts)\n",
      "print \"total_count:\", total_count\n",
      "print \"len(labels):\", len(labels)\n",
      "print \"len(probs):\", len(probs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "len(tag_rows_index): 8149\n",
        "len(tag_counts): 8149\n",
        "total_count: 11266903\n",
        "len(labels): 8149\n",
        "len(probs): 8149\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Optimized O(N)-time, O(1)-auxiliary-space sample algorithm\n",
      "# http://stackoverflow.com/questions/6482889/get-random-sample-from-list-while-maintaining-ordering-of-items\n",
      "def inplace_n_choose(seq, k):\n",
      "    pick_count = 0\n",
      "    for i, val in enumerate(seq):\n",
      "        prob = (k - pick_count) / (len(seq) - i)\n",
      "        if np.random.random() < prob:\n",
      "            yield val\n",
      "            pick_count += 1\n",
      "\n",
      "def neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index):\n",
      "    neg_indices_gen_lst = []\n",
      "    for label in labels:\n",
      "        k, right_lim = np.sum(neg_tags == label), tag_counts[label]\n",
      "        while k > right_lim:\n",
      "            neg_indices_gen_lst.append(inplace_n_choose(tag_rows_index[label], right_lim))\n",
      "            k -= right_lim\n",
      "        neg_indices_gen_lst.append(inplace_n_choose(tag_rows_index[label], k))\n",
      "        for neg_indices_gen in neg_indices_gen_lst:\n",
      "            for neg_index in neg_indices_gen:\n",
      "                yield neg_index\n",
      "\n",
      "def build_instances(doc_lst, indices):\n",
      "    instances = [] \n",
      "    for ind in indices:\n",
      "        instances.append(doc_lst[ind])\n",
      "    return instances"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Views:\n",
      "    tag_rows_index (sorted)\n",
      "    tags_index\n",
      "    tag_counts\n",
      "    labels\n",
      "    probs\n",
      "    bodies_lst"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random stream passes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "init_train_cmd_template = \"vw --loss_function hinge -l 0.1 --l2 0.1 -f ../working/vw/%s.model -k --cache_file ../working/vw/%s.cache\"\n",
      "incr_train_cmd_template = \"vw --loss_function hinge -l 0.1 --l2 0.1 -i ../working/vw/%s.model\" + \\\n",
      "\t\" -f ../working/vw/%s.model\"\n",
      "passes = 5\n",
      "\n",
      "for quitter, tag in enumerate(tags_index):\n",
      "    #if quitter == 2: continue\n",
      "    if quitter == 1: break\n",
      "    \n",
      "    curr_tag = tag\n",
      "    ti = time.time()\n",
      "    # For each tag in the top M tags, choose Q = O/2 '+' examples, where O is occurence count\n",
      "    O = tag_counts[tag] # or, O = len(top_tag_rows_index[tag])\n",
      "    Q = O\n",
      "    ##Call `inplace_n_choose(tag_rows_index[tag], Q)` later\n",
      "    \n",
      "    # For each tag in the top M tags, choose P = 25*Q '-' examples\n",
      "    P = Q\n",
      "#     while Q+P > 0.8*N:\n",
      "#         Q //= 2\n",
      "#         P //= 2\n",
      "    neg_tags = np.random.choice(labels, size=P, p=probs)\n",
      "    \n",
      "    ## If '+' tag, re-sample\n",
      "    uhohs = neg_tags == tag\n",
      "    while np.sum(uhohs) != 0:\n",
      "        neg_tags[uhohs] = np.random.choice(labels, size=np.sum(uhohs), p=probs)\n",
      "        uhohs = neg_tags == tag\n",
      "    ##Call `neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index)` later\n",
      "    \n",
      "    # Build '+' and '-' instances\n",
      "    #pos_indices = inplace_n_choose(tag_rows_index[tag], Q)\n",
      "    pos_indices = tag_rows_index[tag]\n",
      "    neg_indices = neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index)\n",
      "    pos_instances = build_instances(bodies_lst, pos_indices)\n",
      "    neg_instances = build_instances(bodies_lst, neg_indices)\n",
      "    tj = time.time()\n",
      "    print \"example sampling:\", tj - ti, \"seconds\"\n",
      "    \n",
      "    t0 = time.time()\n",
      "    # Build the initial model file\n",
      "    init_train_cmd_str = init_train_cmd_template % (tag, tag) # cache for debug\n",
      "    cmd = shlex.split(init_train_cmd_str)\n",
      "    vw = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "\n",
      "    for instance in pos_instances:\n",
      "    \tvw.stdin.write('1 | ' + instance + '\\n')\n",
      "    for instance in neg_instances:\n",
      "    \tvw.stdin.write('-1 | ' + instance + '\\n')\n",
      "    vw.stdin.flush()\n",
      "    vw.stdin.close()\n",
      "    vw.wait()\n",
      "    #vw.kill()\n",
      "    t1 = time.time()\n",
      "    print \"initial model file built:\", t1 - t0, \"seconds\"\n",
      "    \n",
      "    # Randomize and multiple passes\n",
      "    for epoch in xrange(passes-1):\n",
      "        ta = time.time()\n",
      "    \t# Shuffle '+'s and '-'s, but keep track which is which\n",
      "    \trandom_binaries = np.array([0]*P + [1]*Q, dtype=np.bool_)\n",
      "    \tnp.random.shuffle(random_binaries)\n",
      "    \tnp.random.shuffle(pos_instances)\n",
      "    \tnp.random.shuffle(neg_instances)\n",
      "    \tpos_iter, neg_iter = iter(pos_instances), iter(neg_instances)\n",
      "\n",
      "    \tincr_train_cmd_str = incr_train_cmd_template % (tag, tag)\n",
      "    \tcmd = shlex.split(incr_train_cmd_str)\n",
      "    \tvw = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "\n",
      "    \tfor b in random_binaries:\n",
      "    \t\tif b:\n",
      "    \t\t\tinstance = pos_iter.next()\n",
      "    \t\t\tvw.stdin.write('1 | ' + instance + '\\n')\n",
      "    \t\telse:\n",
      "    \t\t\tinstance = neg_iter.next()\n",
      "    \t\t\tvw.stdin.write('-1 | ' + instance + '\\n')\n",
      "        \n",
      "        vw.stdin.flush()\n",
      "        vw.stdin.close()\n",
      "        vw.wait()\n",
      "        #vw.kill()\n",
      "        tb = time.time()\n",
      "        print \"Epoch %d:\" % (epoch+2), tb - ta, \"seconds\"\n",
      "        \n",
      "    t2 = time.time()\n",
      "    print \"total vw training time:\", t2 - t0, \"seconds\"\n",
      "    '''\n",
      "    '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "example sampling: 87.3992102146 seconds\n",
        "initial model file built:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.02034306526 seconds\n",
        "Epoch 2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.0838971138 seconds\n",
        "Epoch 3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.07249403 seconds\n",
        "Epoch 4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.0858271122 seconds\n",
        "Epoch 5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.07194900513 seconds\n",
        "total vw training time: 10.3380200863 seconds\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create cache for debugging/benchmarking purposes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "curr_tag = tag_rows_index.keys()[0]\n",
      "P = tag_counts[curr_tag]\n",
      "Q = P\n",
      "ti = time.time()\n",
      "neg_tags = np.random.choice(labels, size=P, p=probs)\n",
      "## If '+' tag, re-sample\n",
      "uhohs = neg_tags == curr_tag\n",
      "while np.sum(uhohs) != 0:\n",
      "    neg_tags[uhohs] = np.random.choice(labels, size=np.sum(uhohs), p=probs)\n",
      "    uhohs = neg_tags == curr_tag\n",
      "##Call `neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index)` later\n",
      "\n",
      "# Build '+' and '-' instances\n",
      "#pos_indices = inplace_n_choose(tag_rows_index[tag], Q)\n",
      "pos_indices = tag_rows_index[curr_tag]\n",
      "neg_indices = neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index)\n",
      "pos_instances = build_instances(bodies_lst, pos_indices)\n",
      "neg_instances = build_instances(bodies_lst, neg_indices)\n",
      "\n",
      "test_instances = pos_instances\n",
      "test_instances.extend(neg_instances)\n",
      "del pos_instances, neg_instances\n",
      "\n",
      "assert len(test_instances) == P + Q\n",
      "test_instances_str = '| ' + '\\n| '.join(test_instances)\n",
      "test_instances_str = test_instances_str + '\\n'\n",
      "del test_instances\n",
      "tj = time.time()\n",
      "print \"selecting test examples:\", tj - ti, \"seconds\"\n",
      "\n",
      "cmd_template = \"vw --quiet -k --cache_file ../working/vw/test.cache\"\n",
      "#mm_preds = np.memmap(\"../data/predictions.mmap\", dtype=np.float32, mode='w+', shape=(N_test,M))\n",
      "\n",
      "for quitter,tag in enumerate(tag_rows_index.iterkeys()): # <-- can parallelize! I hope...\n",
      "    if quitter == 0: continue\n",
      "    if quitter == 2: break\n",
      "        \n",
      "    ta = time.time()\n",
      "    cmd_str = cmd_template\n",
      "    cmd = shlex.split(cmd_str)\n",
      "    vw = subprocess.Popen(cmd, stdin=subprocess.PIPE)\n",
      "    vw.communicate(input=test_instances_str);\n",
      "    vw.wait()\n",
      "    tb = time.time()\n",
      "    print \"vw caching time:\", tb - ta, \"seconds\"\n",
      "    # do something with the predictions\n",
      "    #mm_preds[:, sometag] = predictions\n",
      "\n",
      "#del mm_preds\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare cache reading vs streaming for predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd_template = \"vw --quiet -t -p /dev/stdout -i ../working/vw/%s.model\"\n",
      "#mm_preds = np.memmap(\"../data/predictions.mmap\", dtype=np.float32, mode='w+', shape=(N_test,M))\n",
      "\n",
      "for quitter,tag in enumerate(tag_rows_index.iterkeys()): # <-- can parallelize! I hope...\n",
      "    if quitter == 0: continue\n",
      "    if quitter == 2: break\n",
      "        \n",
      "    ta = time.time()\n",
      "    cmd_str = cmd_template % tag\n",
      "    cmd = shlex.split(cmd_str)\n",
      "    vw = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "    stream_out_tup = vw.communicate(input=test_instances_str)\n",
      "    predictions = [np.float32(result) for result in stream_out_tup[0].split('\\n')[:-1]]\n",
      "    vw.wait()\n",
      "    tb = time.time()\n",
      "    print \"vw non-cache-prediction time:\", tb - ta, \"seconds\"\n",
      "    # do something with the predictions\n",
      "    #mm_preds[:, sometag] = predictions\n",
      "\n",
      "#del mm_preds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vw non-cache-prediction time: 3.24497509003 seconds\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd_template = \"vw --quiet -t -p /dev/stdout --cache_file ../working/vw/%s.cache -i ../working/vw/%s.model\"\n",
      "\n",
      "for quitter,tag in enumerate(tag_rows_index.iterkeys()):\n",
      "    if quitter == 0: continue\n",
      "    if quitter == 2: break\n",
      "    \n",
      "    ta = time.time()\n",
      "    cmd_str = cmd_template % (tag, tag)\n",
      "    cmd = shlex.split(cmd_str)\n",
      "    vw = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
      "    predictions = vw.stdout.read()\n",
      "    vw.stdout.flush()\n",
      "    vw.stdout.close()\n",
      "    predictions = predictions.split('\\n')[:-1]\n",
      "    predictions = [np.float32(pred) for pred in predictions]\n",
      "    tb = time.time()\n",
      "    print \"vw cache-prediction time:\", tb - ta, \"seconds\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vw cache-prediction time: 0.222330093384 seconds\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preds = predictions\n",
      "counter = 0\n",
      "for i in xrange(len(preds)//2):\n",
      "    if preds[i] > 0:\n",
      "        counter += 1\n",
      "\n",
      "counter2 = 0\n",
      "for i in xrange(len(preds)//2, len(preds)):\n",
      "    if preds[i] > 0:\n",
      "        counter2 += 1\n",
      "\n",
      "p = counter / (counter + (len(preds)/2 - counter2))\n",
      "r = counter / (counter + (len(preds)/2 - counter))\n",
      "f2 = 2*p*r/(p+r)\n",
      "\n",
      "print \"p:\", p\n",
      "print \"r:\", r\n",
      "print \"F2:\", f2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cached non-shuffled passes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_cmd_template = \"vw --loss_function hinge --passes %d -f ../working/vw/%s.model\" + \\\n",
      "\t\" -k --cache_file ../working/vw/%s.cache --quiet -l 0.1 --l2 0.1\"\n",
      "passes = 20\n",
      "\n",
      "for quitter, tag in enumerate(tag_rows_index.iterkeys()):\n",
      "    if quitter == 1: break\n",
      "    \n",
      "    ti = time.time()\n",
      "    # For each tag in the top M tags, choose Q = O/2 '+' examples, where O is occurence count\n",
      "    O = tag_counts[tag] # or, O = len(top_tag_rows_index[tag])\n",
      "    Q = O // 2\n",
      "    ##Call `inplace_n_choose(tag_rows_index[tag], Q)` later\n",
      "    \n",
      "    # For each tag in the top M tags, choose P = 25*Q '-' examples\n",
      "    P = Q\n",
      "    while Q+P > 0.8*N:\n",
      "        Q //= 2\n",
      "        P //= 2\n",
      "    neg_tags = np.random.choice(labels, size=P, p=probs)\n",
      "    ## If '+' tag, re-sample\\\n",
      "    uhohs = neg_tags == tag\n",
      "    while np.sum(uhohs) != 0:\n",
      "        neg_tags[uhohs] = np.random.choice(labels, size=np.sum(uhohs), p=probs)\n",
      "        uhohs = neg_tags == tag\n",
      "    ##Call `neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index)` later\n",
      "    \n",
      "    # Build '+' and '-' instances\n",
      "    pos_indices = inplace_n_choose(tag_rows_index[tag], Q)\n",
      "    neg_indices = neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index)\n",
      "    pos_instances = build_instances(bodies_lst, pos_indices)\n",
      "    neg_instances = build_instances(bodies_lst, neg_indices)\n",
      "    tj = time.time()\n",
      "    print \"example sampling:\", tj - ti, \"seconds\"    \n",
      "    \n",
      "    t0 = time.time()\n",
      "    train_cmd_str = train_cmd_template % (passes, tag, tag)\n",
      "    cmd = shlex.split(train_cmd_str)\n",
      "    vw = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "\n",
      "    for instance in pos_instances:\n",
      "    \tvw.stdin.write('1 | ' + instance + '\\n')\n",
      "    for instance in neg_instances:\n",
      "    \tvw.stdin.write('-1 | ' + instance + '\\n')\n",
      "    vw.stdin.flush()\n",
      "    vw.stdin.close()\n",
      "    vw.wait()\n",
      "    #vw.kill()\n",
      "    t1 = time.time()\n",
      "    print \"cached passes vw training time:\", t1 - t0, \"seconds\"\n",
      "    #os.remove('../working/vw/%s.cache' % tag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing reference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "ti = time.time()\n",
      "pos_indices = inplace_n_choose(tag_rows_index[tag], Q)\n",
      "neg_indices = neg_indices_gen_helper(neg_tags, labels, tag_counts, tag_rows_index)\n",
      "pos_instances = build_instances(bodies_lst, pos_indices)\n",
      "neg_instances = build_instances(bodies_lst, neg_indices)\n",
      "test_instances = []\n",
      "for instance in pos_instances:\n",
      "    test_instances.append(instance)\n",
      "for instance in neg_instances:\n",
      "    test_instances.append(instance)\n",
      "test_instances_str = '| ' + '\\n| '.join(test_instances)\n",
      "test_instances_str = test_instances_str[:-1] + '\\n'\n",
      "del test_instances\n",
      "tj = time.time()\n",
      "print \"selecting examples:\", tj - ti, \"seconds\"\n",
      "\n",
      "cmd_template = \"vw -p /dev/stdout --quiet -i ../working/vw/%s.model\"\n",
      "#mm_preds = np.memmap(\"../data/predictions.mmap\", dtype=np.float32, mode='w+', shape=(N_test,M))\n",
      "\n",
      "for quitter,tag in enumerate(tag_rows_index.iterkeys()): # <-- can parallelize! I hope...\n",
      "    if quitter == 1: break\n",
      "        \n",
      "    ta = time.time()\n",
      "    cmd_str = cmd_template % tag\n",
      "    cmd = shlex.split(cmd_str)\n",
      "    vw = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "    stream_out_tup = vw.communicate(input=test_instances_str)\n",
      "    predictions = [np.float32(result) for result in stream_out_tup[0].split('\\n')[:-1]]\n",
      "    vw.wait()\n",
      "    tb = time.time()\n",
      "    print \"vw predicting time:\", tb - ta, \"seconds\"\n",
      "    # do something with the predictions\n",
      "    #mm_preds[:, sometag] = predictions\n",
      "\n",
      "#del mm_preds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scoring"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a tags co-occurrence view."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags_co_occurrence = defaultdict(defaultdict(int))\n",
      "for tags_str in tags_lst:\n",
      "    tags = tags_str.split()\n",
      "    for tag in tags:\n",
      "        for tag2 in tags:\n",
      "            if tag != tag2:\n",
      "                tags_co_occurrence[tag][tag2] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M2 = 100 # or len(tags_index) // 2\n",
      "top_tags_set = set(tags_index[:M2])\n",
      "tags_co_occurrence = defaultdict(defaultdict(int))\n",
      "\n",
      "for tags_str in tags_lst:\n",
      "    tags = tags_str.split()\n",
      "    for tag in tags:\n",
      "        if tag in top_tags_set:\n",
      "            for tag2 in tags:\n",
      "                if tag != tag2:\n",
      "                    tags_co_occurrence[tag][tag2] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run through the co_occurrence matrix. If the co-occurrence rate for the top `M2` tags is higher than 0.42 (a little bit over the square root of 2), then automatically include that tag. This is done because the F-score can only go up above when above 0.42."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set the prediction thresholds\n",
      "pred_threshold = 0.8\n",
      "co_occurrence_threshold = 0.42 # baseline.. should only go up when configuring this number\n",
      "tp, fp, fn = 0, 0, 0\n",
      "mm_preds = np.memmap(\"../data/predictions.mmap\", dtype=np.float32, mode='r+', shape=(N, M))\n",
      "tags_index = tag_rows_index.keys()\n",
      "\n",
      "for i in xrange(N):\n",
      "    mm_pred_row = mm_preds[i]\n",
      "    pred_row = np.empty_like(mm_pred_row, dtype=mm_pred_row.dtype)\n",
      "    pred_row[:] = mm_pred_row[:]\n",
      "    # Select the top 5 tags that are over the threshold\n",
      "    top5_top_indices = sorted(np.where(pred_row > pred_threshold))[:5]\n",
      "    top5_top_tags = []\n",
      "    for ind in top5_top_indices:\n",
      "        top5_top_tags.append(tags_index[ind])\n",
      "    # If the selected tags number less than 5, include co-occurring tags above the 0.42 threshold\n",
      "    top_len = len(top5_top_tags)\n",
      "    if top_len < 5:\n",
      "        remainder_len = 5 - top_len\n",
      "        for r,s_tag in enumerate(top5_top_tags):\n",
      "            if r == remainder_len: break\n",
      "            co_tag_dict = tags_co_occurrence[s_tag]\n",
      "            if len(co_tag_dict) != 0:\n",
      "                for (co_tag,co_occurrence) in co_tag_dict.iteritems():\n",
      "                    if co_occurrence / tag_counts[s_tag] >= co_occurrence_threshold:\n",
      "                        top5_top_tags.append(co_tag)\n",
      "    \n",
      "    true_tags = tags_lst[i].split()\n",
      "    \n",
      "    tp_set = set(top5_top_tags) & set(true_tags)\n",
      "    fp_set = set(top5_top_tags) - set(true_tags)\n",
      "    fn_set = set(true_tags) - set(top5_top_tags)\n",
      "    tp += len(tp_set)\n",
      "    fp += len(fp_set)\n",
      "    fn += len(fn_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}