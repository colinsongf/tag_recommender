{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Duplicate Removal"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TOTAL_SIZE = 0\n",
      "title_tags_dict = dict()\n",
      "fin = open(\"../data/Train.csv\", 'rb')\n",
      "fout = open(\"../data/Train_no_dupes.csv\", 'wb')\n",
      "with fin, fout:\n",
      "    rdr = csv.reader(fin, delimiter=',')\n",
      "    wtr = csv.writer(fout, delimiter=',')\n",
      "    # Ignore headers\n",
      "    headers = rdr.next()\n",
      "    \n",
      "    for row in rdr:\n",
      "        if row[1] not in title_set:\n",
      "            TOTAL_SIZE += 1\n",
      "            title_tags_dict[row[1]] = row[-1]\n",
      "            wtr.writerow(row)\n",
      "            \n",
      "print \"TOTAL_SIZE:\", TOTAL_SIZE\n",
      "with open('../working/TOTAL_SIZE.pickle', 'w') as picklefile:\n",
      "    pickle.dump(TOTAL_SIZE, picklefile)\n",
      "    \n",
      "with open(\"../working/title_tags_dict.pickle\", 'w') as picklefile:\n",
      "    pickle.dump(title_tags_dict, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Benchmark Solution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_tags_dict = dict()\n",
      "\n",
      "with open(\"../data/Train_no_dupes.csv\", 'rb') as infile:\n",
      "    rdr = csv.reader(infile)\n",
      "    # headers already omitted\n",
      "    for row in rdr:\n",
      "        if row[1] not in title_tags_dict:\n",
      "            title_tags_dict[row[1]] = row[-1]\n",
      "            \n",
      "# Save title_tags_dict\n",
      "with open(\"../working/title_tags_dict.pickle\", 'w') as picklefile:\n",
      "    pickle.dump(title_tags_dict, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = open(\"../data/submission.csv\", 'wb')\n",
      "infile = open(\"../data/Test.csv\", 'rb')\n",
      "with outfile, infile:\n",
      "    rdr = csv.reader(infile)\n",
      "    # Ignore rdr headers\n",
      "    ignored_headers = rdr.next()\n",
      "    # Write in \"Id\",\"Tags\" as header of submission\n",
      "    outfile.write('\"Id\",\"Tags\"\\n')\n",
      "    for row in rdr:\n",
      "        if row[1] in title_tags_dict:\n",
      "            outfile.write(row[0])\n",
      "            outfile.write(',\"' + title_tags_dict[row[1]] + '\"\\n')\n",
      "        else:\n",
      "            outfile.write(row[0])\n",
      "            outfile.write(',\"javascript c# python php java\"\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Hybrid Solution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_tags_dict = dict()\n",
      "\n",
      "with open(\"../data/Train_no_dupes.csv\", 'rb') as infile:\n",
      "    rdr = csv.reader(infile)\n",
      "    # headers already omitted\n",
      "    for row in rdr:\n",
      "        if row[1] not in title_tags_dict:\n",
      "            title_tags_dict[row[1]] = row[-1]\n",
      "            \n",
      "# Save title_tags_dict\n",
      "with open(\"../working/title_tags_dict.pickle\", 'w') as picklefile:\n",
      "    pickle.dump(title_tags_dict, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "in_train_id_set = set()\n",
      "not_in_train_id_set = set()\n",
      "\n",
      "infile = open(\"../data/Test.csv\", 'rb')\n",
      "outfile = open(\"../data/Test_notrain.csv\", 'wb')\n",
      "\n",
      "with infile, outfile:\n",
      "    rdr = csv.reader(infile)\n",
      "    wtr = csv.writer(outfile)\n",
      "    # skip headers\n",
      "    headers = rdr.next()\n",
      "    for row in rdr:\n",
      "        if row[1] in title_tags_dict:\n",
      "            in_train_id_set.add(row[0])\n",
      "        else:\n",
      "            not_in_train_id_set.add(row[0])\n",
      "            wtr.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, I'm going to treat `Test_notrain.csv` as a `Train.csv` file. Only difference is the vowpal wabbit file, in which I'm not training. Rather, I am testing and then gathering predictions!"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "1. NLTK process it (rewrite)\n",
      "2. Extract only its body to a test_bodyonly.txt file\n",
      "2.5 Figure out the number of rows. Call this \"test_N\"\n",
      "3. Write one .vw test file (no labels)\n",
      "4. Run vw on this one file, but use different model files (m=0..299)\n",
      "5. For each m, there is a prediction file, which can be fitted into a 1D array of size test_N called W. Rather, we want 'p_test' number of T_part, which blah blah blah. Just look at the corresponding code in PLST.ipynb\n",
      "6. Create an inverse-dictionary of tagsdict, called \"inverse_tagsdict\". (tagsdict_id2token)\n",
      "7. Open a submission.csv file and enumerate over it. If line number is in in_train_id_set, then load its pre-loaded tags. If line number isn't in in_train_id_set, then read off a row in T_part, inverse_tagdict each index whose corresponding value is 1, then spit those tokens the row in the csv file!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Basically, just create a carbon-copy of PLST.ipynb and just readjust for testing parameters, and testing namespaces"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}