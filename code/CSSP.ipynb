{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 2.35e4\n",
      "K = 4e4\n",
      "M = 4e2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.ones((N,K), dtype=np.float64)\n",
      "b = np.ones((K,M), dtype=np.float32)\n",
      "c = a.dot(b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bytes_per_float = np.float32(0).nbytes  # ugly, I know\n",
      "aa = np.frombuffer(random.bytes(N * K * bytes_per_float), dtype=np.float32)\n",
      "a = aa.reshape((N,K))\n",
      "del aa\n",
      "print a[0,:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  2.00940290e-33  -3.78433460e-30  -6.62581494e+03  -5.81697110e+15\n",
        "   1.12890518e+04   4.17475016e+21  -2.70072891e-25  -6.93386493e+22\n",
        "  -2.77356475e+06   3.15611698e-02]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b1 = np.ones((K,M), dtype=np.float32)\n",
      "b2 = np.ones((K,M), dtype=np.float32)\n",
      "b3 = np.ones((K,M), dtype=np.float32)\n",
      "b4 = np.ones((K,M), dtype=np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Latest Rewrite\n",
      "##Preprocessing - Natural Language\n",
      "Create a toyset. A toyset in this case is `1/16`th the size of the original data set. This toyset is for testing purposes. It is a subsampled portion of the original data for faster processing and testing times."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TOTAL_SIZE = 6034195\n",
      "SPLIT_SIZE = TOTAL_SIZE / 4\n",
      "toyset_rows = SPLIT_SIZE / 4\n",
      "\n",
      "def split_csv(filename='../data/toyset.csv', delimiter=',', num_splits=4,\n",
      "    fout_template='toyset_%s.csv', output_path='../data/', keep_headers=False,\n",
      "    num_rows=toyset_rows):\n",
      "    \"\"\"\n",
      "    Split a CSV file into multiple pieces.\n",
      "    \n",
      "    Arguments:\n",
      "        `num_splits`: Number of split output files. Default 4\n",
      "        `fout_template`: %s-style template. If not default, will not\n",
      "            append to filename\n",
      "        `output_path`: path of the output split files\n",
      "        `keep_headers`: whether or not to keep the headers in each split file\n",
      "    \"\"\"\n",
      "    # Figure out the # rows each split has and store these into row_limits\n",
      "    row_limit = num_rows / num_splits\n",
      "    row_limits = [row_limit for x in xrange(num_splits-1)]\n",
      "    row_limits.append(num_rows - row_limit*num_splits + row_limit)\n",
      "    # Set the i/o paths\n",
      "    import os\n",
      "    with open(filename, 'rb') as f:\n",
      "        reader = csv.reader(f, delimiter=delimiter)\n",
      "        curr_piece = 0\n",
      "        curr_out_path = os.path.join(\n",
      "            output_path,\n",
      "            fout_template % curr_piece\n",
      "        )\n",
      "        fout = open(curr_out_path, 'wb')\n",
      "        curr_out_writer = csv.writer( fout,\n",
      "                                      delimiter=delimiter )\n",
      "        headers = reader.next()\n",
      "        if keep_headers:\n",
      "            curr_out_writer.write(headers)\n",
      "        else:\n",
      "            reader.next();\n",
      "            \n",
      "        curr_limit = row_limits[curr_piece]\n",
      "        for i, row in enumerate(reader):\n",
      "            if i >= curr_limit:\n",
      "                curr_piece += 1\n",
      "                if curr_piece != num_splits:\n",
      "                    curr_limit += row_limits[curr_piece]\n",
      "                    curr_out_path = os.path.join(\n",
      "                        output_path,\n",
      "                        fout_template % curr_piece\n",
      "                    )\n",
      "                    fout.close()\n",
      "                    fout = open(curr_out_path, 'wb')\n",
      "                    curr_out_writer = csv.writer(fout, delimiter=delimiter)\n",
      "                    if keep_headers:\n",
      "                        curr_out_writer.writerow(headers)\n",
      "                else:\n",
      "                    fout.close()\n",
      "                    break\n",
      "            curr_out_writer.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "TOTAL_SIZE = 6034195\n",
      "SPLIT_SIZE = TOTAL_SIZE / 4\n",
      "toyset_rows = SPLIT_SIZE / 4\n",
      "\n",
      "# Split the toyset further into 4 splits for parallel processing\n",
      "try:\n",
      "    test_stream = open(\"../data/toyset_0.csv\", \"rb\")\n",
      "    test_stream.close()\n",
      "    del test_stream\n",
      "except:\n",
      "    split_csv(keep_headers=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup the IPython engines for parallel preprocessing. Make sure you have already turned on the IPython cluster engines. I'm assuming you have 4 engines for 4 processors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "client = Client()\n",
      "lv = client.load_balanced_view()\n",
      "dv = client[:]\n",
      "print lv\n",
      "print dv\n",
      "\n",
      "engine_ids = range(4)\n",
      "for eid in engine_ids:\n",
      "    client[eid]['eid'] = eid\n",
      "    client[eid]['leftoff'] = 0\n",
      "print dv['eid']\n",
      "print dv['leftoff']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import nltk, csv, re\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "infilename = \"../data/toyset_%d.csv\" % eid\n",
      "outfilename = \"../data/proc_toyset_%d.csv\" % eid\n",
      "TOTAL_SIZE = 6034195\n",
      "SPLIT_SIZE = TOTAL_SIZE / 4\n",
      "toyset_rows = SPLIT_SIZE / 4\n",
      "split_rows = toyset_rows / 4\n",
      "\n",
      "# Stop List according to Treebank-POS tags\n",
      "REJECT = set([\"CC\", \"DT\", \"EX\", \"FW\", \"IN\", \"LS\", \"MD\", \"PDT\",\n",
      "    \"PRP\", \"PRP$\", \"RP\", \"TO\", \"UH\", \"WDT\", \"WP\", \"WP$\", \"WRB\",\n",
      "    \"JJR\", \"JJS\", \"CD\", \"RB\", \"RBR\", \"RBS\", \"POS\", \"VBP\"])\n",
      "# REFERENCE: Accept List\n",
      "ACCEPT = set([\"JJ\", \"NN\", \"NNP\", \"NNS\", \"NNPS\", \"VB\", \"VBG\", \"VBN\",\n",
      "                   \"VBZ\"])\n",
      "# Additional Stop-words list\n",
      "sw = set([])\n",
      "with open(\"../working/english.stop\", 'rb') as f:\n",
      "    for l in f:\n",
      "        sw.add(l.rstrip())\n",
      "# Tokenizer customized for StackOverflow posts\n",
      "tokenizer = RegexpTokenizer(r'[^\\W_\\-]+\\'?[^\\W_\\-]+[+#]{,2}')\n",
      "# Brill Tagger\n",
      "import pickle\n",
      "tagger = pickle.load(open(\"../working/treebank_brill_aubt.pickle\"))\n",
      "# WordNet Lemmatizer\n",
      "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
      "# Convert treebank POS to WordNet POS\n",
      "wntag_dict = {'J':'a', 'V':'v', 'N':'n', 'R': 'r'}\n",
      "tb2wn = lambda x: wntag_dict[x[0]] if x[0] in wntag_dict else 'n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "def process_title(title):\n",
      "    # Get rid of appended punctuations\n",
      "    tokens = tokenizer.tokenize(title)\n",
      "    tktag_tups = tagger.tag(tokens)\n",
      "    # Filter out tokens that have irrelevant POS tags\n",
      "    filtered_tups = [(tup[0].lower(), tup[1]) for tup in tktag_tups\n",
      "                     if tup[1] not in REJECT]\n",
      "    #filtered_tups = filter(lambda tup: tup[1] not in REJECT, tktag_tups)\n",
      "    filtered_tups = filter(lambda tup: re.match(r\"\\D+\", tup[0]), filtered_tups)\n",
      "    # Lemma + lower (normalize) tokens\n",
      "    lemmaed_tks = [lemmatizer.lemmatize(t[0],tb2wn(t[1]))\n",
      "                   for t in filtered_tups if t[0] not in sw] \n",
      "    concat_tks = re.sub(r\"\\'\", '', ' '.join(lemmaed_tks))\n",
      "    return concat_tks.encode('ascii', errors='ignore')\n",
      "\n",
      "def process_body(body):\n",
      "    # Parse HTML tags\n",
      "    soup = BeautifulSoup(body)\n",
      "    body_lst = [unicode(tag.string) for tag in soup.find_all('p')]\n",
      "    #body_lst.extend([unicode(tag.string) for tag in soup.find_all('code')])\n",
      "    # Parse out pre-pended symbol tags (except C#)\n",
      "    tokens_lst = tokenizer.batch_tokenize(body_lst)\n",
      "    tokens = []\n",
      "    for tl in tokens_lst:\n",
      "        tokens.extend(tl)\n",
      "    # Filter out tokens that have irrelevant POS tags\n",
      "    tktag_tups = tagger.tag(tokens)\n",
      "    filtered_tups = [(tup[0].lower(), tup[1]) for tup in tktag_tups\n",
      "                     if tup[1] not in REJECT]\n",
      "    #filtered_tups = filter(lambda tup: tup[1] not in REJECT, tktag_tups)\n",
      "    filtered_tups = filter(lambda tup: re.match(r\"\\D+\", tup[0]), filtered_tups)\n",
      "    # Lemma + lower (normalize) tokens\n",
      "    lemmaed_tks = [lemmatizer.lemmatize(t[0],tb2wn(t[1]))\n",
      "                   for t in filtered_tups if t[0] not in sw]\n",
      "    concat_tks = re.sub(r\"\\'\", '', ' '.join(lemmaed_tks))\n",
      "    return concat_tks.encode('ascii', errors='ignore')\n",
      "\n",
      "def process_csv(infilename=infilename, outfilename=outfilename,\n",
      "                leftoff=leftoff):\n",
      "    fin = open(infilename, \"rb\")\n",
      "    inreader = csv.reader(fin, delimiter=\",\")\n",
      "    # Ignore header\n",
      "    inreader.next();\n",
      "    \n",
      "    if leftoff > 0:\n",
      "        fout = open(outfilename, \"ab\")\n",
      "    else:\n",
      "        fout = open(outfilename, \"wb\")\n",
      "    outreader = csv.writer(fout, delimiter=\",\")\n",
      "    batch_num = split_rows / 10\n",
      "    for i,row in enumerate(inreader):\n",
      "        if i < leftoff: continue\n",
      "        if (i % batch_num == 0):\n",
      "            print \"Percent completed: %.2f\" % (i / batch_num * 10.0)\n",
      "        newRow = []\n",
      "        # Pass id\n",
      "        newRow.append(row[0])\n",
      "        # Process title\n",
      "        title = row[1].decode('ascii', 'ignore')\n",
      "        '''\n",
      "        try:\n",
      "            process_title(title)\n",
      "        except:\n",
      "            print \"Error\"\n",
      "            print \"title.__repr__():\", title.__repr__()\n",
      "            print \"title.__str__():\", title.__str__()\n",
      "        '''\n",
      "        newTitle = process_title(title)#.encode('ascii', 'ignore')\n",
      "        newRow.append(newTitle)\n",
      "        # Process body\n",
      "        body = row[2].decode('ascii', 'ignore')\n",
      "        '''\n",
      "        try:\n",
      "            process_body(body)\n",
      "        except:\n",
      "            print \"Error\"\n",
      "            print \"body.__repr__():\", body.__repr__()\n",
      "            print \"body.__str__():\", body.__str__()\n",
      "        '''\n",
      "        newBody = process_body(body)#.encode('ascii', 'ignore')\n",
      "        newRow.append(newBody)\n",
      "        # Pass tags\n",
      "        newRow.append(row[-1].decode('ascii', 'ignore'))\n",
      "        # Write to output file\n",
      "        outreader.writerow(newRow)\n",
      "    fin.close()\n",
      "    fout.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##DEBUGGGGG\n",
      "process_csv(infilename='../data/toyset_0.csv', outfilename='../data/proc_toyset_0.csv',\n",
      "            leftoff=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = 'Do search engines respect the HTTP header field \\xe2\\x80\\x9cContent-Location\\xe2\\x80\\x9d?'\n",
      "a.decode('ascii', 'ignore').encode('ascii', 'ignore')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "# Might want to skip this block...\n",
      "with open(\"../data/proc_toyset_%d.csv\" % eid, \"rb\") as f:\n",
      "    count = 0\n",
      "    rdr = csv.reader(f)\n",
      "    for row in rdr:\n",
      "        count += 1\n",
      "    print count\n",
      "    leftoff = count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run this block ONCE\n",
      "ars = []\n",
      "for eid in engine_ids:\n",
      "    ars.append(client[eid].execute(\"process_csv()\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ar = ars[1]\n",
      "ar.metadata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ar.elapsed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`DEBUG`: Check if the preprocessing works as intended."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px --targets=1\n",
      "print eid\n",
      "cutoff = 10\n",
      "with open(\"../data/proc_toyset_0.csv\", 'rb') as f:\n",
      "    rdr = csv.reader(f, delimiter=',')\n",
      "    for i,row in enumerate(rdr):\n",
      "        if i == cutoff: break\n",
      "        test_str = row[2]\n",
      "        tkns = tokenizer.tokenize(test_str)\n",
      "        #print tkns\n",
      "        tags = tagger.tag(tkns)\n",
      "        #print tags\n",
      "        filtered_tags = [(tup[0].lower(), tup[1]) for tup in tags\n",
      "                         if tup[1] not in REJECT]\n",
      "        #print filtered_tags\n",
      "        lemmaed_tks = [lemmatizer.lemmatize(t[0], tb2wn(t[1]))\n",
      "                       for t in filtered_tags if t[0] not in sw]\n",
      "        print \" \".join(lemmaed_tks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Merge the 4 splits back into one processed toyset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm ../data/proc_toyset.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# No need for parallel processing as the HDD is the bottleneck\n",
      "for eid in engine_ids:\n",
      "    fin = open(\"../data/proc_toyset_%d.csv\" % eid, \"rb\")\n",
      "    fout = open(\"../data/proc_toyset.csv\", \"ab\")\n",
      "    with fin, fout:\n",
      "        rdr = csv.reader(fin, delimiter=',')\n",
      "        wtr = csv.writer(fout, delimiter=',')\n",
      "        for row in rdr:\n",
      "            wtr.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remove duplicate rows. `proc_toyset.csv` refers to the original processed toyset with duplicates. `proc2_toyset.csv` refers to the duplicate-free processed toyset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titleDict = {}\n",
      "fin = open(\"../data/proc_toyset.csv\", 'rb')\n",
      "with fin:\n",
      "    rdr = csv.reader(fin, delimiter=',')\n",
      "    for row in rdr:\n",
      "        if row[1] in titleDict:\n",
      "            titleDict[row[1]].intersection_update(set(row[3]))\n",
      "        else:\n",
      "            titleDict[row[1]] = set(row[3].split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(titleDict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dupSet = set([])\n",
      "fin = open(\"../data/proc_toyset.csv\", \"rb\")\n",
      "fout = open(\"../data/proc2_toyset.csv\", \"wb\")\n",
      "with fin, fout:\n",
      "    rdr = csv.reader(fin, delimiter=',')\n",
      "    wtr = csv.writer(fout, delimiter=',')\n",
      "    for row in rdr:\n",
      "        if row[1] in dupSet: continue\n",
      "        dupSet.add(row[1])\n",
      "        newRow = row[:-1]\n",
      "        newRow.append(\" \".join(titleDict[row[1]]))\n",
      "        wtr.writerow(newRow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc ../data/proc2_toyset.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc ../data/proc_toyset.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Free up some memory\n",
      "del titleDict\n",
      "del dupSet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Parallel</b>\n",
      "\n",
      "Prepare for parallelization. Create a helper script which splits a file for parallelization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "proc_toyset_rows = 364379\n",
      "\n",
      "filename = '../data/proc2_toyset.csv'\n",
      "fout_template = '%sonly_%d.txt'\n",
      "num_splits = 4\n",
      "output_path='../data/'\n",
      "num_rows = proc_toyset_rows\n",
      "delimiter = ','\n",
      "\n",
      "# Figure out the # of rows each split has and store these into row_limits\n",
      "print 'num_rows:', num_rows\n",
      "rough_limit = num_rows / num_splits\n",
      "row_limits = [rough_limit for x in xrange(num_splits-1)]\n",
      "row_limits.append(num_rows - rough_limit*num_splits + rough_limit)\n",
      "\n",
      "import os\n",
      "import csv\n",
      "\n",
      "curr_piece = 0\n",
      "with open(filename, 'rb') as fin:\n",
      "    rdr = csv.reader(fin, delimiter=delimiter)\n",
      "    for row_limit in row_limits:\n",
      "        print 'row_limit:', row_limit\n",
      "        curr_out_path1 = os.path.join(\n",
      "            output_path,\n",
      "            fout_template % ('title',curr_piece)\n",
      "        )\n",
      "        curr_out_path2 = os.path.join(\n",
      "            output_path,\n",
      "            fout_template % ('body',curr_piece)\n",
      "        )\n",
      "        curr_out_path3 = os.path.join(\n",
      "            output_path,\n",
      "            fout_template % ('tags',curr_piece)\n",
      "        )\n",
      "        fout1 = open(curr_out_path1, 'wb')\n",
      "        fout2 = open(curr_out_path2, 'wb')\n",
      "        fout3 = open(curr_out_path3, 'wb')\n",
      "        \n",
      "        for x in xrange(row_limit):\n",
      "            rdr.next()\n",
      "            fout1.write(row[1] + \"\\n\")\n",
      "            fout2.write(row[2] + \"\\n\")\n",
      "            fout3.write(row[3] + \"\\n\")\n",
      "            \n",
      "        fout1.close() # cannot imbed 'with' statements within 'with' statements\n",
      "        fout2.close()\n",
      "        fout3.close()\n",
      "        curr_piece += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print row_limits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Serial</b>\n",
      "\n",
      "Extract only bodies from `proc2_toyset.csv`. No splits.This and the following versions are an alternative to the above code step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fin = open('../data/proc2_toyset.csv', 'rb')\n",
      "fout = open('../data/bodyonly.txt', 'wb')\n",
      "with fin, fout:\n",
      "    rdr = csv.reader(fin, delimiter=',')\n",
      "    for row in rdr:\n",
      "        fout.write(row[2])\n",
      "        fout.write('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract only titles from `proc2_toyset.csv`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fin = open(\"../data/proc2_toyset.csv\", 'rb')\n",
      "fout = open('../data/titleonly.txt', 'wb')\n",
      "with fin, fout:\n",
      "    rdr = csv.reader(fin, delimiter=',')\n",
      "    for row in rdr:\n",
      "        fout.write(row[1])\n",
      "        fout.write('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract only tags from `proc2_toyset.csv`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fin = open(\"../data/proc2_toyset.csv\", 'rb')\n",
      "fout = open('../data/tagsonly.txt', 'wb')\n",
      "with fin, fout:\n",
      "    rdr = csv.reader(fin, delimiter=',')\n",
      "    for row in rdr:\n",
      "        fout.write(row[3])\n",
      "        fout.write('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 ../data/bodyonly.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Dimensionality Reduction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "client = Client()\n",
      "lv = client.load_balanced_view()\n",
      "dv = client[:]\n",
      "print lv\n",
      "print dv\n",
      "\n",
      "engine_ids = range(4)\n",
      "for eid in engine_ids:\n",
      "    client[eid]['eid'] = eid\n",
      "    client[eid]['leftoff'] = 0\n",
      "print dv['eid']\n",
      "print dv['leftoff']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Body only"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "from gensim.models.lsimodel import LsiModel\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "from gensim.corpora.hashdictionary import HashDictionary\n",
      "\n",
      "import mmh3 # murmurhash v3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform a percentile reduction (aka filter out 'insignificant' words)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    bodydict = Dictionary.load('../working/bodydict.pickle')\n",
      "    print \"loaded bodydict\"\n",
      "except:\n",
      "    # Build my dictionary\n",
      "    bodydict = Dictionary(unicode(line, errors='ignore').split() for line in\n",
      "                        open('../data/bodyonly.txt', 'rb'))\n",
      "    # Filter out 'insignificant' words\n",
      "    bodydict.filter_extremes(no_below=2, no_above=0.5)\n",
      "    # Save bodydict\n",
      "    bodydict.save(\"../working/bodydict.pickle\")\n",
      "    print \"created bodydict\"\n",
      "    \n",
      "n_unique_words = int(bodydict.__str__().lstrip('Dictionary(').rstrip(\n",
      "    ' unique tokens)'))\n",
      "print bodydict\n",
      "print 'n_unique_words:', n_unique_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "created bodydict\n",
        "Dictionary(59740 unique tokens)\n",
        "n_unique_words: 59740\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Serial</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    q = open('../data/filtered_bodyonly.txt', 'rb')\n",
      "    q.close()\n",
      "    print 'filtered_bodyonly.txt created already'\n",
      "except:\n",
      "    fin = open('../data/bodyonly.txt', 'rb')\n",
      "    fout = open('../data/filtered_bodyonly.txt', 'wb')\n",
      "    with fin, fout:\n",
      "        for line in fin:\n",
      "            fout.write(' '.join([tkn for tkn in line.split()\n",
      "                                 if tkn in bodydict.token2id]) + \"\\n\")\n",
      "del bodydict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Parallel</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import gensim\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "\n",
      "try:\n",
      "    bodydict = Dictionary.load('../working/bodydict.pickle')\n",
      "    print \"loaded bodydict\"\n",
      "except:\n",
      "    print \"bodydict.pickle is not created yet...\"\n",
      "\n",
      "try:\n",
      "    q = open('../data/filtered_bodyonly_%d.txt' % eid, 'rb')\n",
      "    q.close()\n",
      "    print 'filtered_bodyonly_%d.txt created already' % eid\n",
      "except:\n",
      "    fin = open('../data/bodyonly_%d.txt' % eid, 'rb')\n",
      "    fout = open('../data/filtered_bodyonly_%d.txt' % eid, 'wb')\n",
      "    with fin, fout:\n",
      "        for line in fin:\n",
      "            fout.write(' '.join([tkn for tkn in line.split()\n",
      "                                 if tkn in bodydict.token2id]) + \"\\n\")\n",
      "del bodydict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc ../data/bodyonly.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc ../data/filtered_bodyonly.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do NOT perform SVD/LSI... yet. Relegate the hashing task onto Vowpal Wabbit."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Title only (a replica of the above step)\n",
      "\n",
      "Perform a percentile reduction (aka filter out 'insignificant' words)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    titledict = Dictionary.load(\"../working/titledict.pickle\")\n",
      "    print \"loaded titledict\"\n",
      "except:\n",
      "    # Build my dictionary\n",
      "    titledict = Dictionary(unicode(line, errors='ignore').split() for line in\n",
      "                        open('../data/titleonly.txt', 'rb'))\n",
      "    # Filter out 'insignificant' words\n",
      "    titledict.filter_extremes(no_below=2, no_above=0.5)\n",
      "    # Save titledict\n",
      "    titledict.save(\"../working/titledict.pickle\")\n",
      "    print \"created titledict\"\n",
      "    \n",
      "n_unique_words = int(titledict.__str__().lstrip(\"Dictionary(\").rstrip(\n",
      "    ' unique tokens)'))\n",
      "print titledict\n",
      "print \"n_unique_words:\", n_unique_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Serial</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    q = open('../data/filtered_titleonly.txt', 'rb')\n",
      "    q.close()\n",
      "    print \"filtered_titleonly.txt created already\"\n",
      "except:\n",
      "    fin = open(\"../data/titleonly.txt\", \"rb\")\n",
      "    fout = open(\"../data/filtered_titleonly.txt\", \"wb\")\n",
      "    with fin, fout:\n",
      "        for line in fin:\n",
      "            fout.write( \" \".join([tkn for tkn in line.split()\n",
      "                                  if tkn in titledict.token2id]) + \"\\n\")\n",
      "            \n",
      "del titledict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Parallel</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import gensim\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "\n",
      "try:\n",
      "    titledict = Dictionary.load(\"../working/titledict.pickle\")\n",
      "    print \"loaded titledict\"\n",
      "except:\n",
      "    print \"titledict.pickle not created yet...\"\n",
      "\n",
      "try:\n",
      "    q = open('../data/filtered_titleonly_%d.txt' % eid, 'rb')\n",
      "    q.close()\n",
      "    print 'filtered_titleonly_%d.txt created already' % eid\n",
      "except:\n",
      "    fin = open('../data/titleonly_%d.txt' % eid, 'rb')\n",
      "    fout = open('../data/filtered_titleonly_%d.txt' % eid, 'wb')\n",
      "    with fin, fout:\n",
      "        for line in fin:\n",
      "            fout.write(' '.join([tkn for tkn in line.split()\n",
      "                                 if tkn in titledict.token2id]) + \"\\n\")\n",
      "del titledict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc ../data/titleonly.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!wc ../data/filtered_titleonly.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do NOT perform SVD/LSI... yet. Relegate the hashing task onto Vowpal Wabbit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "id_range = 2**int(ceil(log2(n_unique_words))) # efficiency\n",
      "hashDict = HashDictionary(id_range=id_range, myhash=mmh3.hash)\n",
      "\n",
      "# Online stream of documents in hashed, binary, bag-of-words format\n",
      "class MyCorpus(object):\n",
      "    def __init__(self, filename):\n",
      "        self.filename = filename\n",
      "    def __iter__(self):\n",
      "        for line in open(self.filename,'rb'):\n",
      "            yield [(tup[0], 1) for tup in hashDict.doc2bow(line.split())]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Tags only (dimensionality reduction with CSSP Randomized Sampling)\n",
      "Pre-filter `tagsonly.txt` to `filtered_tagsonly.txt`. Obtain only `V`, the singular vectors for `filtered_tagsonly.txt` (tags). Treat the `V` matrix as a matrix of probabilities from which to sample up to `k` dimensions. Encode `filtered_tagsonly.txt` to `filtered2_tagsonly.txt` using the `k` vectors.\n",
      "\n",
      "Reset memory and load necessary modules."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset -f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "from gensim.models.lsimodel import LsiModel\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "from gensim.corpora.hashdictionary import HashDictionary\n",
      "\n",
      "import mmh3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform a percentile reduction (aka filter out 'insignificant' tags). This step is not meant for dimensionality reduction. Rather, it is a step that precedes the Lsi step to save time in deciding which tag is important or not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    tagsdict = Dictionary.load('../working/tagsdict.pickle')\n",
      "    print \"loaded tagsdict\"\n",
      "except:\n",
      "    # Build my dictionary\n",
      "    tagsdict = Dictionary(unicode(line, errors='ignore').split() for line in\n",
      "                        open('../data/tagsonly.txt', 'rb'))\n",
      "    # Filter out 'insignificant' tags\n",
      "    tagsdict.filter_extremes(no_below=2)\n",
      "    # Save tagsdict\n",
      "    tagsdict.save(\"../working/tagsdict.pickle\")\n",
      "    print \"created tagsdict\"\n",
      "    \n",
      "n_unique_words = int(tagsdict.__str__().lstrip('Dictionary(').rstrip(\n",
      "    ' unique tokens)'))\n",
      "print tagsdict\n",
      "print \"n_unique_words:\", n_unique_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "created tagsdict\n",
        "Dictionary(20610 unique tokens)\n",
        "n_unique_words: 20610\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Serial</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    q = open(\"../data/filtered_tagsonly.txt\", \"rb\")\n",
      "    q.close()\n",
      "    print \"filtered_tagsonly.txt created already\"\n",
      "except:\n",
      "    fin = open(\"../data/tagsonly.txt\", \"rb\")\n",
      "    fout = open(\"../data/filtered_tagsonly.txt\", \"wb\")\n",
      "    with fin, fout:\n",
      "        for line in fin:\n",
      "            fout.write( \" \".join([tkn for tkn in line.split()\n",
      "                                  if tkn in tagsdict.token2id]) + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Parallel</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import gensim\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "\n",
      "try:\n",
      "    tagsdict = Dictionary.load('../working/tagsdict.pickle')\n",
      "    print \"loaded tagsdict\"\n",
      "except:\n",
      "    print \"tagsdict.pickle not created yet...\"\n",
      "\n",
      "try:\n",
      "    q = open('../data/filtered_tagsonly_%d.txt' % eid, 'rb')\n",
      "    q.close()\n",
      "    print 'filtered_tagsonly_%d.txt created already' % eid\n",
      "except:\n",
      "    fin = open('../data/tagsonly_%d.txt' % eid, 'rb')\n",
      "    fout = open('../data/filtered_tagsonly_%d.txt' % eid, 'wb')\n",
      "    with fin, fout:\n",
      "        for line in fin:\n",
      "            fout.write(' '.join([tkn for tkn in line.split()\n",
      "                                 if tkn in tagsdict.token2id]) + \"\\n\")\n",
      "del tagsdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stitch back up the files together."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm ../data/filtered_tagsonly.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I/O must be serialized, but the below SVD/LSI process is batched/parallelized.\n",
      "for eid in engine_ids:\n",
      "    fin = open(\"../data/filtered_tagsonly_%d.txt\" % eid, \"rb\")\n",
      "    fout = open(\"../data/filtered_tagsonly.txt\", \"ab\")\n",
      "    with fin, fout:\n",
      "        fout.write(fin.readline() + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SVD/LSI step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Online stream of documents in hashed, binary, bag-of-words format\n",
      "class MyCorpus(object):\n",
      "    def __init__(self, filename):\n",
      "        self.filename = filename\n",
      "    def __iter__(self):\n",
      "        for line in open(self.filename,'rb'):\n",
      "            yield [(tup[0], 1) for tup in tagsdict.doc2bow(line.split())]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# dimensionality reduction\n",
      "ndim = 100 # increase if possible later on...\n",
      "\n",
      "corpus = MyCorpus('../data/filtered_tagsonly.txt')\n",
      "model = LsiModel(num_topics=ndim, id2word=tagsdict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "model.add_documents(corpus, chunksize=40000, decay=0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:36,246 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:37,094 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:37,576 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:37,585 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:38,056 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:39,233 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,453 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,453 : INFO : keeping 100 factors (discarding 15.335% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,500 : INFO : processed documents up to #40000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,510 : INFO : topic #0(57.141): 0.690*\"c#\" + 0.455*\"javascript\" + 0.324*\"jquery\" + 0.278*\"php\" + 0.202*\".net\" + 0.156*\"asp.net\" + 0.140*\"html\" + 0.115*\"java\" + 0.073*\"css\" + 0.067*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,513 : INFO : topic #1(55.493): -0.626*\"c#\" + 0.506*\"javascript\" + 0.373*\"jquery\" + 0.348*\"php\" + -0.184*\".net\" + 0.157*\"html\" + -0.086*\"asp.net\" + 0.082*\"css\" + 0.065*\"mysql\" + 0.058*\"ajax\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,515 : INFO : topic #2(51.796): -0.919*\"java\" + -0.317*\"android\" + 0.117*\"javascript\" + 0.098*\"jquery\" + -0.079*\"php\" + 0.076*\"c#\" + -0.053*\"swing\" + -0.042*\"eclipse\" + -0.035*\"mysql\" + -0.034*\"spring\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,517 : INFO : topic #3(50.428): -0.849*\"php\" + 0.359*\"javascript\" + 0.257*\"jquery\" + -0.208*\"mysql\" + 0.142*\"java\" + -0.055*\"sql\" + 0.051*\"css\" + 0.049*\"html\" + 0.042*\"android\" + -0.036*\"arrays\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:41,519 : INFO : topic #4(42.998): 0.911*\"android\" + -0.318*\"java\" + 0.170*\"iphone\" + 0.120*\"ios\" + 0.110*\"objective-c\" + 0.041*\"android-layout\" + 0.035*\"xcode\" + 0.032*\"sqlite\" + 0.028*\"listview\" + -0.027*\"swing\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:42,398 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:42,878 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:42,884 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:43,302 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:44,207 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:45,944 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:45,944 : INFO : keeping 100 factors (discarding 15.633% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:45,986 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:46,243 : INFO : keeping 100 factors (discarding 0.666% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:46,298 : INFO : processed documents up to #80000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:46,299 : INFO : topic #0(63.792): 0.601*\"javascript\" + 0.463*\"jquery\" + 0.419*\"c#\" + 0.346*\"php\" + 0.182*\"html\" + 0.146*\"java\" + 0.130*\"asp.net\" + 0.118*\".net\" + 0.098*\"css\" + 0.081*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:46,321 : INFO : topic #1(61.502): -0.827*\"c#\" + 0.316*\"javascript\" + 0.252*\"jquery\" + -0.228*\".net\" + 0.208*\"php\" + -0.163*\"asp.net\" + 0.092*\"html\" + -0.056*\"wpf\" + 0.053*\"css\" + -0.051*\"winforms\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:46,322 : INFO : topic #2(58.971): 0.912*\"java\" + 0.341*\"android\" + -0.113*\"javascript\" + -0.099*\"jquery\" + -0.098*\"c#\" + 0.056*\"swing\" + 0.042*\"eclipse\" + 0.032*\"spring\" + -0.032*\"asp.net\" + 0.030*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:46,335 : INFO : topic #3(56.021): 0.869*\"php\" + -0.332*\"javascript\" + -0.242*\"jquery\" + 0.224*\"mysql\" + -0.087*\"java\" + 0.066*\"sql\" + -0.042*\"css\" + 0.029*\"codeigniter\" + -0.029*\"android\" + 0.028*\"database\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:46,345 : INFO : topic #4(48.989): -0.928*\"android\" + 0.346*\"java\" + -0.060*\"iphone\" + -0.047*\"android-layout\" + -0.047*\"ios\" + -0.034*\"objective-c\" + 0.031*\"swing\" + -0.030*\"listview\" + -0.022*\"android-intent\" + -0.019*\"sqlite\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:47,190 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:47,668 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:47,676 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:48,089 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:49,009 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:50,743 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:50,743 : INFO : keeping 100 factors (discarding 15.671% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:50,796 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:51,049 : INFO : keeping 100 factors (discarding 0.702% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:51,108 : INFO : processed documents up to #120000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:51,125 : INFO : topic #0(65.829): 0.711*\"c#\" + 0.434*\"javascript\" + 0.337*\"jquery\" + 0.226*\"php\" + 0.200*\".net\" + 0.177*\"asp.net\" + 0.134*\"html\" + 0.124*\"java\" + 0.074*\"css\" + 0.059*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:51,144 : INFO : topic #1(63.995): -0.601*\"c#\" + 0.530*\"javascript\" + 0.426*\"jquery\" + 0.294*\"php\" + -0.170*\".net\" + 0.153*\"html\" + -0.090*\"asp.net\" + 0.090*\"css\" + 0.061*\"ajax\" + 0.060*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:51,151 : INFO : topic #2(60.424): 0.903*\"java\" + 0.375*\"android\" + -0.103*\"javascript\" + -0.098*\"jquery\" + -0.073*\"c#\" + 0.052*\"swing\" + 0.048*\"eclipse\" + 0.034*\"spring\" + 0.029*\"hibernate\" + 0.025*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:51,173 : INFO : topic #3(57.425): 0.883*\"php\" + -0.302*\"javascript\" + 0.234*\"mysql\" + -0.228*\"jquery\" + -0.070*\"java\" + 0.058*\"sql\" + -0.041*\"css\" + -0.038*\"android\" + 0.030*\"arrays\" + 0.026*\"wordpress\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:51,187 : INFO : topic #4(51.035): 0.917*\"android\" + -0.381*\"java\" + 0.038*\"android-layout\" + 0.037*\"iphone\" + 0.032*\"ios\" + -0.031*\"swing\" + 0.025*\"listview\" + 0.023*\"android-intent\" + 0.021*\"objective-c\" + 0.021*\"sqlite\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:52,055 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:52,533 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:52,539 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:52,953 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:53,885 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:55,624 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:55,625 : INFO : keeping 100 factors (discarding 15.675% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:55,674 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:55,928 : INFO : keeping 100 factors (discarding 0.611% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:55,983 : INFO : processed documents up to #160000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:56,002 : INFO : topic #0(66.529): 0.792*\"c#\" + 0.359*\"javascript\" + 0.269*\"jquery\" + 0.218*\".net\" + 0.187*\"php\" + 0.174*\"asp.net\" + 0.115*\"java\" + 0.110*\"html\" + 0.058*\"css\" + 0.055*\"wpf\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:56,020 : INFO : topic #1(64.548): 0.591*\"javascript\" + -0.498*\"c#\" + 0.453*\"jquery\" + 0.326*\"php\" + 0.172*\"html\" + -0.138*\".net\" + 0.095*\"css\" + 0.077*\"java\" + 0.071*\"ajax\" + 0.068*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:56,026 : INFO : topic #2(61.272): -0.908*\"java\" + -0.360*\"android\" + 0.118*\"javascript\" + 0.100*\"jquery\" + 0.061*\"c#\" + -0.050*\"swing\" + -0.044*\"eclipse\" + -0.034*\"spring\" + -0.033*\"hibernate\" + 0.025*\"html\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:56,051 : INFO : topic #3(57.729): 0.880*\"php\" + -0.315*\"javascript\" + 0.236*\"mysql\" + -0.221*\"jquery\" + -0.064*\"java\" + 0.062*\"sql\" + -0.038*\"css\" + 0.031*\"database\" + 0.030*\"arrays\" + -0.028*\"android\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:56,062 : INFO : topic #4(51.071): -0.920*\"android\" + 0.366*\"java\" + -0.066*\"iphone\" + -0.052*\"ios\" + -0.041*\"objective-c\" + -0.036*\"android-layout\" + 0.030*\"swing\" + -0.025*\"listview\" + -0.024*\"sqlite\" + -0.020*\"android-intent\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:56,960 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:57,438 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:57,446 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:57,858 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:17:58,762 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,497 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,499 : INFO : keeping 100 factors (discarding 15.598% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,542 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,799 : INFO : keeping 100 factors (discarding 0.694% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,855 : INFO : processed documents up to #200000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,874 : INFO : topic #0(66.257): 0.728*\"c#\" + 0.403*\"javascript\" + 0.317*\"jquery\" + 0.260*\"php\" + 0.194*\".net\" + 0.179*\"asp.net\" + 0.126*\"html\" + 0.122*\"java\" + 0.068*\"css\" + 0.064*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,878 : INFO : topic #1(64.548): -0.585*\"c#\" + 0.513*\"javascript\" + 0.411*\"jquery\" + 0.368*\"php\" + -0.155*\".net\" + 0.154*\"html\" + -0.093*\"asp.net\" + 0.085*\"css\" + 0.073*\"mysql\" + 0.065*\"ajax\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,907 : INFO : topic #2(61.333): -0.910*\"java\" + -0.357*\"android\" + 0.113*\"javascript\" + 0.099*\"jquery\" + 0.060*\"c#\" + -0.058*\"swing\" + -0.042*\"eclipse\" + -0.036*\"spring\" + -0.032*\"hibernate\" + 0.027*\"html\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,917 : INFO : topic #3(58.101): -0.847*\"php\" + 0.366*\"javascript\" + 0.279*\"jquery\" + -0.221*\"mysql\" + 0.058*\"java\" + -0.056*\"sql\" + 0.048*\"css\" + 0.043*\"html\" + -0.033*\"arrays\" + -0.028*\"database\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:00,944 : INFO : topic #4(51.842): -0.922*\"android\" + 0.362*\"java\" + -0.061*\"iphone\" + -0.048*\"ios\" + -0.039*\"android-layout\" + -0.038*\"objective-c\" + 0.033*\"swing\" + -0.028*\"sqlite\" + -0.024*\"listview\" + -0.022*\"android-intent\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:01,835 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:02,312 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:02,319 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:02,733 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:03,677 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,423 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,424 : INFO : keeping 100 factors (discarding 15.519% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,477 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,726 : INFO : keeping 100 factors (discarding 0.644% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,782 : INFO : processed documents up to #240000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,799 : INFO : topic #0(66.579): 0.565*\"c#\" + 0.515*\"javascript\" + 0.421*\"jquery\" + 0.319*\"php\" + 0.165*\"html\" + 0.154*\".net\" + 0.147*\"java\" + 0.144*\"asp.net\" + 0.082*\"css\" + 0.079*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,821 : INFO : topic #1(64.747): -0.744*\"c#\" + 0.403*\"javascript\" + 0.332*\"jquery\" + 0.277*\"php\" + -0.205*\".net\" + 0.123*\"html\" + -0.122*\"asp.net\" + 0.064*\"css\" + 0.057*\"mysql\" + -0.051*\"wpf\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,842 : INFO : topic #2(61.801): -0.907*\"java\" + -0.355*\"android\" + 0.120*\"javascript\" + 0.111*\"jquery\" + 0.070*\"c#\" + -0.067*\"swing\" + -0.043*\"eclipse\" + -0.033*\"spring\" + 0.026*\"html\" + -0.024*\"hibernate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,862 : INFO : topic #3(58.921): -0.858*\"php\" + 0.334*\"javascript\" + 0.274*\"jquery\" + -0.231*\"mysql\" + 0.065*\"java\" + -0.064*\"sql\" + 0.043*\"css\" + 0.040*\"html\" + 0.031*\"android\" + -0.031*\"arrays\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:05,878 : INFO : topic #4(52.245): -0.925*\"android\" + 0.362*\"java\" + 0.038*\"swing\" + -0.037*\"android-layout\" + -0.031*\"iphone\" + -0.029*\"listview\" + -0.024*\"ios\" + -0.022*\"sqlite\" + -0.021*\"android-intent\" + 0.017*\"spring\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:06,783 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:07,282 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:07,289 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:07,704 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:08,621 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,385 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,385 : INFO : keeping 100 factors (discarding 15.602% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,438 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,746 : INFO : keeping 100 factors (discarding 0.603% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,803 : INFO : processed documents up to #280000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,806 : INFO : topic #0(67.314): 0.686*\"c#\" + 0.442*\"javascript\" + 0.343*\"jquery\" + 0.274*\"php\" + 0.187*\".net\" + 0.171*\"asp.net\" + 0.140*\"html\" + 0.137*\"java\" + 0.077*\"css\" + 0.076*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,834 : INFO : topic #1(65.302): -0.634*\"c#\" + 0.486*\"javascript\" + 0.385*\"jquery\" + 0.344*\"php\" + -0.174*\".net\" + 0.151*\"html\" + -0.096*\"asp.net\" + 0.087*\"css\" + 0.079*\"mysql\" + 0.054*\"ajax\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,835 : INFO : topic #2(62.162): -0.915*\"java\" + -0.341*\"android\" + 0.114*\"javascript\" + 0.104*\"jquery\" + 0.067*\"c#\" + -0.062*\"swing\" + -0.044*\"eclipse\" + -0.035*\"spring\" + 0.027*\"html\" + 0.026*\"asp.net\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,868 : INFO : topic #3(59.257): -0.843*\"php\" + 0.361*\"javascript\" + 0.272*\"jquery\" + -0.251*\"mysql\" + -0.063*\"sql\" + 0.059*\"java\" + 0.052*\"css\" + 0.037*\"html\" + -0.028*\"arrays\" + -0.027*\"codeigniter\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:10,869 : INFO : topic #4(51.940): -0.931*\"android\" + 0.346*\"java\" + -0.041*\"android-layout\" + -0.039*\"iphone\" + 0.035*\"swing\" + -0.028*\"ios\" + -0.028*\"listview\" + -0.024*\"sqlite\" + -0.024*\"android-intent\" + -0.022*\"objective-c\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:11,746 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:12,226 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:12,233 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:12,648 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:13,598 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,346 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,348 : INFO : keeping 100 factors (discarding 15.517% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,403 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,654 : INFO : keeping 100 factors (discarding 0.561% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,713 : INFO : processed documents up to #320000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,721 : INFO : topic #0(67.319): 0.567*\"javascript\" + 0.481*\"c#\" + 0.436*\"jquery\" + 0.347*\"php\" + 0.181*\"html\" + 0.147*\"java\" + 0.135*\".net\" + 0.131*\"asp.net\" + 0.097*\"mysql\" + 0.093*\"css\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,756 : INFO : topic #1(65.241): -0.798*\"c#\" + 0.349*\"javascript\" + 0.283*\"jquery\" + 0.243*\"php\" + -0.225*\".net\" + -0.129*\"asp.net\" + 0.108*\"html\" + 0.059*\"css\" + 0.054*\"mysql\" + -0.053*\"wpf\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,769 : INFO : topic #2(62.439): -0.895*\"java\" + -0.386*\"android\" + 0.109*\"javascript\" + 0.099*\"jquery\" + 0.092*\"c#\" + -0.058*\"swing\" + -0.052*\"eclipse\" + -0.030*\"spring\" + 0.028*\"asp.net\" + -0.027*\"hibernate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,792 : INFO : topic #3(59.724): -0.844*\"php\" + 0.342*\"javascript\" + -0.273*\"mysql\" + 0.268*\"jquery\" + -0.065*\"sql\" + 0.061*\"java\" + 0.046*\"html\" + 0.044*\"css\" + -0.034*\"arrays\" + 0.033*\"android\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:15,815 : INFO : topic #4(52.601): -0.911*\"android\" + 0.393*\"java\" + -0.041*\"android-layout\" + 0.036*\"swing\" + -0.036*\"iphone\" + -0.031*\"listview\" + -0.028*\"sqlite\" + -0.027*\"ios\" + -0.024*\"android-intent\" + 0.019*\"spring\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:16,727 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:17,207 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:17,214 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:17,630 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:18,575 : INFO : 2nd phase: running dense svd on (200L, 40000L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,332 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,334 : INFO : keeping 100 factors (discarding 15.582% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,381 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,644 : INFO : keeping 100 factors (discarding 0.647% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,709 : INFO : processed documents up to #360000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,733 : INFO : topic #0(67.861): 0.616*\"javascript\" + 0.479*\"jquery\" + 0.396*\"c#\" + 0.346*\"php\" + 0.191*\"html\" + 0.124*\"asp.net\" + 0.104*\".net\" + 0.099*\"java\" + 0.094*\"css\" + 0.088*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,747 : INFO : topic #1(65.494): -0.846*\"c#\" + 0.294*\"javascript\" + 0.235*\"jquery\" + -0.224*\".net\" + 0.195*\"php\" + -0.152*\"asp.net\" + 0.086*\"html\" + -0.058*\"wpf\" + -0.054*\"winforms\" + 0.045*\"css\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,783 : INFO : topic #2(61.690): -0.905*\"java\" + -0.357*\"android\" + 0.101*\"javascript\" + 0.091*\"jquery\" + -0.087*\"php\" + 0.079*\"c#\" + -0.058*\"swing\" + -0.050*\"eclipse\" + -0.045*\"mysql\" + -0.035*\"spring\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,809 : INFO : topic #3(59.889): -0.859*\"php\" + 0.318*\"javascript\" + -0.254*\"mysql\" + 0.240*\"jquery\" + 0.137*\"java\" + -0.063*\"sql\" + 0.050*\"android\" + 0.036*\"css\" + -0.032*\"c#\" + 0.032*\"html\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,822 : INFO : topic #4(52.032): -0.923*\"android\" + 0.363*\"java\" + -0.045*\"iphone\" + -0.043*\"android-layout\" + 0.033*\"swing\" + -0.033*\"ios\" + -0.031*\"listview\" + -0.027*\"android-intent\" + -0.026*\"sqlite\" + -0.026*\"objective-c\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:20,977 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:21,055 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:21,088 : INFO : 1st phase: constructing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:21,157 : INFO : orthonormalizing (20610L, 200L) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:21,948 : INFO : 2nd phase: running dense svd on (200L, 4379L) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,075 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,075 : INFO : keeping 100 factors (discarding 16.353% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,131 : INFO : merging projections: (20610L, 100L) + (20610L, 100L)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,388 : INFO : keeping 100 factors (discarding 2.117% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,447 : INFO : processed documents up to #364379\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,473 : INFO : topic #0(39.149): 0.591*\"c#\" + 0.524*\"javascript\" + 0.408*\"jquery\" + 0.279*\"php\" + 0.168*\"asp.net\" + 0.167*\".net\" + 0.165*\"html\" + 0.099*\"java\" + 0.086*\"css\" + 0.073*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,500 : INFO : topic #1(37.934): -0.718*\"c#\" + 0.446*\"javascript\" + 0.351*\"jquery\" + 0.251*\"php\" + -0.205*\".net\" + 0.134*\"html\" + -0.124*\"asp.net\" + 0.072*\"css\" + 0.050*\"mysql\" + -0.050*\"wpf\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,525 : INFO : topic #2(35.557): -0.912*\"java\" + -0.355*\"android\" + 0.094*\"javascript\" + 0.088*\"jquery\" + -0.061*\"swing\" + 0.057*\"c#\" + -0.052*\"php\" + -0.046*\"eclipse\" + -0.035*\"spring\" + -0.034*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,539 : INFO : topic #3(34.344): -0.874*\"php\" + 0.302*\"javascript\" + -0.248*\"mysql\" + 0.233*\"jquery\" + 0.102*\"java\" + -0.066*\"sql\" + 0.038*\"android\" + 0.037*\"css\" + -0.033*\"arrays\" + -0.031*\"codeigniter\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-12-13 20:18:22,558 : INFO : topic #4(29.727): -0.921*\"android\" + 0.358*\"java\" + -0.072*\"iphone\" + -0.050*\"ios\" + -0.048*\"android-layout\" + -0.043*\"objective-c\" + 0.035*\"swing\" + -0.031*\"listview\" + -0.026*\"android-intent\" + -0.025*\"activity\"\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print shape(model.projection.u)\n",
      "# Save the (dense) matrix\n",
      "np.save(\"../working/Vtags.npy\", model.projection.u)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(20610L, 100L)\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculation of probabilities and random sampling step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "V = np.load(\"../working/Vtags.npy\")\n",
      "VT = V.T\n",
      "print \"V.T's shape:\", VT.shape\n",
      "n_unique_tags = V.shape[0]\n",
      "\n",
      "probabilities = empty(n_unique_tags) # <-- shape from V.T.shape\n",
      "for i in xrange(n_unique_tags):\n",
      "    probabilities[i] = (1./ndim) * np.linalg.norm(VT[:,i],2) ** 2\n",
      "    \n",
      "samples = np.random.choice(n_unique_tags, ndim, p=probabilities,\n",
      "    replace=False) #<-- hmm, paper seems suspicious about this point\n",
      "np.save('../working/samples.npy', samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "V.T's shape: (100L, 20610L)\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "from collections import OrderedDict\n",
      "\n",
      "if 'samples' not in globals():\n",
      "\tsamples = np.load('../working/samples.npy')\n",
      "tagsdict = Dictionary.load('../working/tagsdict.pickle')\n",
      "sampled_tagsdict = OrderedDict()\n",
      "counter = 0\n",
      "for hash_id, word_token in tagsdict.iteritems():\n",
      "\tif hash_id in samples:\n",
      "\t\tsampled_tagsdict[word_token] = counter\n",
      "\t\tcounter += 1\n",
      "        \n",
      "import pickle\n",
      "pickle.dump(sampled_tagsdict, open('../working/sampled_tagsdict.pickle', 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Encode `filtered_tagsonly.txt` to `filtered2_tagsonly.txt` using the `k` vectors. This essentially means filtering out (for the 2nd time) tags that were NOT sampled. If a row/item does not contain any sampled tags, it carries the default tag: `$NA$`. `$NA$` will be passed into vowpal wabbit and classified, but we will later disregard `$NA$` result as an error. More on this later..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fin = open(\"../data/filtered_tagsonly.txt\", 'rb')\n",
      "fout= open(\"../data/filtered2_tagsonly.txt\", 'wb')\n",
      "\n",
      "with fin, fout:\n",
      "    for line in fin:\n",
      "        tkn_lst = []\n",
      "        for tkn in line.split():\n",
      "            if tagsdict.token2id[tkn] in samples:\n",
      "                tkn_lst.append(tkn)\n",
      "        if tkn_lst != []:\n",
      "            fout.write(\" \".join(tkn_lst) + \"\\n\")\n",
      "        else:\n",
      "            fout.write(\"$NA$\\n\") # fill with text \"$NA$\" iff no label exists"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import pickle\n",
      "with open(\"../working/sampled_tagsdict.pickle\", 'r') as pickle_file:\n",
      "    sampled_tagsdict = pickle.load(pickle_file)\n",
      "infilename = '../data/filtered_tagsonly_%d.txt' % eid\n",
      "outfilename = '../data/filtered2_tagsonly_%d.txt' % eid\n",
      "infile = open(infilename1, 'rb')\n",
      "outfile = open(outfilename, 'wb')\n",
      "with infile, outfile:\n",
      "    for line in infile:\n",
      "        tkn_lst = []\n",
      "        for tkn in line.split():\n",
      "            if tagsdict.token2id[tkn] in samples:\n",
      "                tkn_lst.append(tkn)\n",
      "        if tkn_lst != []:\n",
      "            fout.write(\" \".join(tkn_lst) + '\\n')\n",
      "        else:\n",
      "            fout.write(\"$NA$\\n\") # fill with text \"$NA$\" iff no label exists\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Cleaning Up\n",
      "###Vowpal Wabbit\n",
      "Create a vowpal_wabbit-friendly format as training input.\n",
      "\n",
      "<b>Serial</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a training file\n",
      "import gensim\n",
      "from collections import OrderedDict\n",
      "\n",
      "TOTAL_SIZE = 6034195\n",
      "SPLIT_SIZE = TOTAL_SIZE / 4\n",
      "toyset_rows = SPLIT_SIZE / 4\n",
      "#split_rows = toyset_rows / 4\n",
      "\n",
      "# May have to batch-ify the following step (suitable only for toyset?)\n",
      "f_bodies = open(\"../data/filtered_bodyonly.txt\", 'rb')\n",
      "#f_titles = open(\"../data/filtered_titleonly.txt\", 'rb')\n",
      "f_tags = open(\"../data/filtered2_tagsonly.txt\", 'rb')\n",
      "f_vw = open(\"../data/Train.vw\", 'wb')\n",
      "\n",
      "import pickle\n",
      "with open('../working/sampled_tagsdict.pickle', 'r') as pickle_file:\n",
      "    sampled_tagsdict = pickle.load(pickle_file)\n",
      "\n",
      "body = 'dummy value' # <-- dummy value\n",
      "l_tags = 'dummy value'\n",
      "with f_tags, f_bodies, f_vw:\n",
      "    while l_tags != '':\n",
      "        l_tags = f_tags.readline()\n",
      "        body = f_bodies.readline()\n",
      "        for tkn in l_tags.split():\n",
      "            for tag in sampled_tagsdict:\n",
      "                # @body decorator is optional, but the body information\n",
      "                # is mandatory.\n",
      "                #if tkn == \"$NA$\":\n",
      "                #    f_vw.write('|%s ' % tag)\n",
      "                    \n",
      "                if tkn == tag:\n",
      "                    f_vw.write('1 100 |%s ' % tag)\n",
      "                    \n",
      "                else:\n",
      "                    f_vw.write('-1 |%s ' % tag)\n",
      "                \n",
      "                # Pass body text directly into the vw file (sparse representation)\n",
      "                # VW will auto-hash/tokenize the features and treat values as binary\n",
      "                f_vw.write(body) # newline included in body already\n",
      "                \n",
      "                # May or may not use titles, depending on the curse of\n",
      "                # dimensionality (signal lost in noise, or too many dimensions\n",
      "                #f_vw.write('|%s@title ' % tag)\n",
      "                \n",
      "                # Pass title text directly into the vw file\n",
      "                #f_vw.write(f_titles.readline())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make a testing file from the training file\n",
      "import gensim\n",
      "from collections import OrderedDict\n",
      "\n",
      "TOTAL_SIZE = 6034195\n",
      "SPLIT_SIZE = TOTAL_SIZE / 4\n",
      "toyset_rows = SPLIT_SIZE / 4\n",
      "#split_rows = toyset_rows / 4\n",
      "\n",
      "# May have to batch-ify the following step (suitable only for toyset?)\n",
      "f_bodies = open(\"../data/filtered_bodyonly.txt\", 'rb')\n",
      "#f_titles = open(\"../data/filtered_titleonly.txt\", 'rb')\n",
      "f_test = open(\"../data/Train.test\", 'wb')\n",
      "\n",
      "import pickle\n",
      "with open('../working/sampled_tagsdict.pickle', 'r') as pickle_file:\n",
      "    sampled_tagsdict = pickle.load(pickle_file)\n",
      "\n",
      "with f_bodies, f_test:\n",
      "    while True:\n",
      "        body = f_bodies.readline()\n",
      "        if body == '':\n",
      "            break\n",
      "        for tag in sampled_tagsdict:\n",
      "            f_test.write('|%s ' % tag)\n",
      "            f_test.write(body) # newline included in body already"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Parallel</b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import gensim\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "from gensim.corpora.hashdictionary import HashDictionary\n",
      "\n",
      "TOTAL_SIZE = 6034195\n",
      "SPLIT_SIZE = TOTAL_SIZE / 4\n",
      "toyset_rows = SPLIT_SIZE / 4\n",
      "#split_rows = toyset_rows / 4\n",
      "\n",
      "# May have to batch-ify the following step (suitable only for toyset?)\n",
      "f_bodies = open(\"../data/filtered_bodyonly_%d.txt\" % eid, 'rb')\n",
      "#f_titles = open(\"../data/filtered_titleonly_%d.txt\" % eid, 'rb')\n",
      "f_tags = open(\"../data/filtered2_tagsonly_%d.txt\" % eid, 'rb')\n",
      "f_vw = open(\"../data/Train_%d.vw\" % eid, 'wb')\n",
      "\n",
      "import pickle\n",
      "with open('../working/sampled_tagsdict.pickle', 'r') as pickle_file:\n",
      "    sampled_tagsdict = pickle.load(pickle_file)\n",
      "\n",
      "with f_tags, f_vw:\n",
      "    for i in xrange(toyset_rows):\n",
      "        l_tags = f_tags.readline()\n",
      "        body = f_bodies.readline()\n",
      "        for tkn in l_tags.split():\n",
      "            for tag in sampled_tagsdict:\n",
      "                # @body decorator is optional, but the body information\n",
      "                # is mandatory.\n",
      "                if tkn == \"$NA$\":\n",
      "                    f_vw.write('|%s ' % tag)\n",
      "                    \n",
      "                elif tkn == tag:\n",
      "                    f_vw.write('1 |%s ' % tag)\n",
      "                    \n",
      "                else:\n",
      "                    f_vw.write('0 |%s ' % tag)\n",
      "                \n",
      "                # Pass body text directly into the vw file (sparse representation)\n",
      "                # VW will auto-hash/tokenize the features and treat values as binary\n",
      "                f_vw.write(body)\n",
      "                \n",
      "                # May or may not use titles, depending on the curse of\n",
      "                # dimensionality (signal lost in noise, or too many dimensions\n",
      "                #f_vw.write('|%s@title ' % tag)\n",
      "                \n",
      "                # Pass title text directly into the vw file\n",
      "                #f_vw.write(f_titles.readline())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, run Vowpal Wabbit. Be cautious of overfitting and class imbalance (skewed data). E.g.\n",
      "\n",
      "<ul>`vw -d Train.vw --cache_file ../working/Train.cache --passes 4 --loss_function squared -f ../working/Train.model`</ul>\n",
      "<ul>`vw -t Train.test --cache_file ../working/Train.test.cache -i ../working/Train.model -p Train.pred`</ul>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's keep track of all data transformations at this point in time...\n",
      "\n",
      "<b>`Y`</b>\n",
      "<ul>Begin: `filtered_tagsonly.txt` file format</ul>\n",
      "<ul>Transform: For each token in the file format, use `tagsdict` gensim-dictionary to `tagsdict.token2id[token]` --> `hash_id/j`, which corresponds to the `jth` index of a matrix. `Y` is also a sparse matrix</ul>\n",
      "<ul>Storage: Save the matrix as `Y.npz`, a binary archive format on the hard drive</ul>\n",
      "\n",
      "<b>`YC.T`</b>\n",
      "<ul>Begin: `filtered2_tagsonly.txt` file format</ul>\n",
      "<ul>Transform: For each token in the file format, use `sampled_tagsdict` ordered-dict to `sampled_tagsdict[token]` --> `hash_id/j`, which corresponds to the `jth` index of a matrix</ul>\n",
      "<ul>Storage: Save the matrix as `YCT.npz`, a binary archive format on the hard drive</ul>\n",
      "\n",
      "<b>`HC`</b>\n",
      "<ul>Begin: `Train.pred` file format</ul>\n",
      "<ul>Description: `Train.pred` is an encoded predictions file in \"Binary Relevance\" form. Convert it back into \"Multi-label\" form.</ul>\n",
      "<ul>Transform: Each line contains a number, which upon rounding, becomes `0` or `1`. Keeping track of the line number and `len(sampled_tagsdict)`, convert from binary-classification form back to multi-label form, but this time in `hash_id` space. Thus, `line-number` is `i`, and `hash_id` is `j`, which are indices of a `sparse-matrix`.</ul>\n",
      "<ul>Storage: Save the matrix as `HC.npz`, a binary archive format on the hard drive</ul>\n",
      "\n",
      "<b>`H`</b>\n",
      "<ul>Begin: `numpy sparse/dense array` - because `H = HC.dot(YC.T).dot(Y)`</ul>\n",
      "<ul>Storage: Save the matrix as `H.npy/npz`, a binary (archive) format on the hard drive</ul>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###<b>`HC`</b>\n",
      "<ul>Begin: `Train.pred` file format</ul>\n",
      "<ul>Description: `Train.pred` is an encoded predictions file in \"Binary Relevance\" form. Convert it back into \"Multi-label\" form.</ul>\n",
      "<ul>Transform: Each line contains a number, which upon rounding, becomes `0` or `1`. Keeping track of the line number and `len(sampled_tagsdict)`, convert from binary-classification form back to multi-label form, but this time in `hash_id` space. Thus, `line-number` is `i`, and `hash_id` is `j`, which are indices of a `sparse-matrix`.</ul>\n",
      "<ul>Storage: Save the matrix as `HC.npz`, a binary archive format on the hard drive</ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred_file = open(\"../data/Train.pred\", 'rb')\n",
      "# Assuming sampled_tagsdict retains the same order as the above code block\n",
      "import pickle\n",
      "from collections import OrderedDict\n",
      "with open('../working/sampled_tagsdict.pickle', 'r') as pickle_file:\n",
      "    sampled_tagsdict = pickle.load(pickle_file)\n",
      "\n",
      "num_rows = 364379 # wc -l ../data/Train.pred\n",
      "ndim = 100 # from before (beware of magic numbers!)\n",
      "import scipy.sparse\n",
      "HC = scipy.sparse.lil_matrix((num_rows, ndim), dtype=bool)\n",
      "\n",
      "with pred_file:\n",
      "    ind = -1 # sorry, it has to start at -1\n",
      "    flag = True\n",
      "    pred_line = pred_file.readline()\n",
      "    while flag:\n",
      "        positive_lst = []\n",
      "        # Assuming sampled_tagsdict retains the same order as the above code block\n",
      "        for tag in sampled_tagsdict:\n",
      "            float_pred_line = -1 # default value\n",
      "            try:\n",
      "                float_pred_line = float(pred_line)\n",
      "            except ValueError: # if pred_line == '' (end-of-file reached)\n",
      "                flag = False\n",
      "                break\n",
      "            if float_pred_line > 0:\n",
      "                positive_lst.append(sampled_tagsdict[tag])\n",
      "            pred_line = pred_file.readline()\n",
      "            ind += 1 # readline() is always one index ahead of 'ind'\n",
      "            \n",
      "        i = ind / ndim\n",
      "        for j in positive_lst:\n",
      "            HC[i, j] = True\n",
      "HC = HC.tocsr()\n",
      "print \"HC:\", HC.__repr__()\n",
      "# Save HC as .npz for offline storage\n",
      "# A csr_matrix has 3 data attributes that matter: .data, .indices, and .indptr.\n",
      "# All are simple ndarrays, so numpy.save will work on them. Save the three arrays with numpy.save\n",
      "# or numpy.savez, load them back with numpy.load, and then recreate the sparse matrix object with:\n",
      "#\n",
      "# new_csr = scipy.sparse.csr_matrix((data, indices, indptr), [shape=(M, N)])\n",
      "np.savez(\"../working/HC.npz\", HC.data, HC.indices, HC.indptr)\n",
      "\n",
      "# To load it back perform the following:\n",
      "#\n",
      "# npzfile = np.load(../working/HC.npz)\n",
      "# HC = scipy.sparse.csr_matrix((npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']))\n",
      "# del npzfile\n",
      "del HC"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "HC: <364379x100 sparse matrix of type '<type 'numpy.bool_'>'\n",
        "\twith 1222572 stored elements in Compressed Sparse Row format>\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###<b>`Y`</b>\n",
      "<ul>Begin: `filtered_tagsonly.txt` file format</ul>\n",
      "<ul>Transform: For each token in the file format, use `tagsdict` gensim-dictionary to `tagsdict.token2id[token]` --> `hash_id/j`, which corresponds to the `jth` index of a matrix. `Y` is also a sparse matrix</ul>\n",
      "<ul>Storage: Save the matrix as `Y.npz`, a binary archive format on the hard drive</ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "with open(\"../working/tagsdict.pickle\", 'rb') as picklefile:\n",
      "    tagsdict = pickle.load(picklefile)\n",
      "\n",
      "n_unique_tags = int(tagsdict.__str__().lstrip('Dictionary(').rstrip(\n",
      "    ' unique tokens)'))\n",
      "\n",
      "num_rows = 364379 # wc -l ../data/filtered_tagsonly.txt\n",
      "import scipy.sparse\n",
      "Y = scipy.sparse.lil_matrix((num_rows, n_unique_tags), dtype=bool)\n",
      "\n",
      "with open(\"../data/filtered_tagsonly.txt\", 'rb') as filtered_tagsonly_file:\n",
      "    for i,line in enumerate(filtered_tagsonly_file):\n",
      "        for token in line.split():\n",
      "            if token in tagsdict.token2id:\n",
      "                Y[i, tagsdict.token2id[token]] = True\n",
      "Y = Y.tocsr()\n",
      "print \"Y:\", Y.__repr__()\n",
      "np.savez(\"../working/Y.npz\", Y.data, Y.indices, Y.indptr)\n",
      "\n",
      "del Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Y: <364379x20610 sparse matrix of type '<type 'numpy.bool_'>'\n",
        "\twith 1015576 stored elements in Compressed Sparse Row format>\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###<b>`YC.T`</b>\n",
      "<ul>Begin: `filtered2_tagsonly.txt` file format</ul>\n",
      "<ul>Transform: For each token in the file format, use `sampled_tagsdict` ordered-dict to `sampled_tagsdict[token]` --> `hash_id/j`, which corresponds to the `jth` index of a matrix</ul>\n",
      "<ul>Storage: Save the matrix as `YCT.npz`, a binary archive format on the hard drive</ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "from collections import OrderedDict\n",
      "with open(\"../working/sampled_tagsdict.pickle\", 'r') as picklefile:\n",
      "    sampled_tagsdict = pickle.load(picklefile)\n",
      "\n",
      "num_rows = 364379 # wc -l ../data/filtered_tagsonly.txt\n",
      "ndim = 100 # from before (beware of magic numbers!)\n",
      "import scipy.sparse\n",
      "YCT = scipy.sparse.lil_matrix((ndim, num_rows), dtype=bool) # remember, we want YCT, not YC\n",
      "\n",
      "with open(\"../data/filtered2_tagsonly.txt\", 'rb') as filtered_tagsonly_file:\n",
      "    for i,line in enumerate(filtered_tagsonly_file):\n",
      "        for token in line.split():\n",
      "            if token in sampled_tagsdict:\n",
      "                YCT[sampled_tagsdict[token],i] = True\n",
      "YCT = YCT.tocsr()\n",
      "print \"YC.T:\", YCT.__repr__()\n",
      "np.savez(\"../working/YCT.npz\", YCT.data, YCT.indices, YCT.indptr)\n",
      "\n",
      "del YCT"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "YC.T: <100x364379 sparse matrix of type '<type 'numpy.bool_'>'\n",
        "\twith 384944 stored elements in Compressed Sparse Row format>\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###<b>`H`</b>\n",
      "<ul>Begin: `numpy sparse/dense array` - because `H = HC.dot(YC.T).dot(Y)`</ul>\n",
      "<ul>Storage: Save the matrix as `H.npy/npz`, a binary (archive) format on the hard drive</ul>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse\n",
      "\n",
      "npzfile = np.load(\"../working/YCT.npz\")\n",
      "YCT = scipy.sparse.csr_matrix((npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']))\n",
      "\n",
      "#-----------------------------------------------------------------------------------------\n",
      "\n",
      "npzfile = np.load(\"../working/Y.npz\")\n",
      "Y = scipy.sparse.csr_matrix((npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']))\n",
      "\n",
      "#-----------------------------------------------------------------------------------------\n",
      "# Perform H = HC.dot(YCT).dot(Y) in a memory-cheap way\n",
      "# This requires the evaluation of YCT.dot(Y) first because its state requires the smallest matrix size\n",
      "# [as compared with HC.dot(YCT)].\n",
      "# Thus HC.dot(temp_matrix1) is better than temp_matrix2.dot(Y)\n",
      "# i.e. (n x k) (k x k) is easier than (n x n) (n x k)\n",
      "\n",
      "temp_matrix = YCT.dot(Y)\n",
      "del YCT, Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npzfile = np.load(\"../working/HC.npz\")\n",
      "HC = scipy.sparse.csr_matrix((npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H = HC.dot(temp_matrix)\n",
      "del temp_matrix, HC\n",
      "np.savez(\"../working/H.npz\", H.data, H.indices, H.indptr)\n",
      "del H"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##F1 Scoring\n",
      "<p>The evaluation metric for this competition is <a href=\"http://en.wikipedia.org/wiki/F1_score\"> Mean F1-Score</a>. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision `p` and recall `r`. Precision is the ratio of true positives `(tp)` to all predicted positives `(tp + fp)`. Recall is the ratio of true positives to all actual positives `(tp + fn)`. The F1 score is given by:</p>\n",
      "\n",
      "$$F1 = 2\\frac{p \\cdot r}{p+r}\\ \\ \\mathrm{where}\\ \\ p = \\frac{tp}{tp+fp},\\ \\ r = \\frac{tp}{tp+fn}$$\n",
      "<p>The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.</p>\n",
      "<p>To receive credit, the tag you predict must be an exact match, regardless of whether you believe the tags are synonyms.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To work around RAM limitations (I only have 8GB ram, out of which I prefer to use only 3/4's of [~6GB], just to be safe), we'll need to batch-process this step as well. This means calculating the true/false positive/negative rates by summing these values in toyset levels, i.e. dividing the training set into 16 toyset-level partitions, and then adding up the rates over these 16 partitions. The `p`, `r`, and `F1` calculations remain unchanged."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Ideal"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tp = np.sum(H==Y)\n",
      "fp = np.sum((H != Y) & (H == 1))\n",
      "fn = np.sum((H != Y) & (H == 0))\n",
      "\n",
      "p = 1.0 * tp / (tp + fp)\n",
      "r = 1.0 * tp / (tp + fn)\n",
      "F1 = 2 * (p * r) / (p + r)\n",
      "\n",
      "print \"true positive:\", tp\n",
      "print \"false positive:\", fp\n",
      "print \"false negative:\", fn\n",
      "print \"precision:\", p\n",
      "print \"recall:\", r\n",
      "print \"F1-score:\", F1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Practical"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll need to text-ify the following matrices\n",
      "<ul>`H == 1`</ul>\n",
      "<ul>`H == 0`</ul>\n",
      "<ul>`H == Y` <-- take the sum to get `tp`</ul>\n",
      "<ul>`H != Y`</ul>\n",
      "<ul>`(H != Y) & (H == 0)` <-- take the sum to get `fp`</ul>\n",
      "<ul>`(H != Y) & (H == 1)` <-- take the sum to get `fn`</ul>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`H == 1`, or just `H`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npzfile = np.load(\"../working/H.npz\")\n",
      "H = scipy.sparse.csr_matrix((npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']))\n",
      "H_nonzeros = H.nonzero()\n",
      "del H\n",
      "H_coords_arr = np.vstack(H_nonzeros)\n",
      "del H_nonzeros\n",
      "np.save(\"../working/H_coords_arr.npy\", H_coords_arr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if 'H_coords_arr' not in globals():\n",
      "    H_coords_arr = np.load(\"../working/H_coords_arr.npy\")\n",
      "\n",
      "with open(\"../working/H.txt\", 'wb') as outfile:\n",
      "    curr_i = -1\n",
      "    for x in np.nditer(H_coords_arr, flags=['external_loop'], order='F'):\n",
      "        if x[0] > curr_i:\n",
      "            curr_i = x[0]\n",
      "            outfile.write('\\n') # quirk: remember to skip the first line...\n",
      "            outfile.write(str(x[0]) + ' ' + str(x[1]) + ' ')\n",
      "        else:\n",
      "            outfile.write(str(x[1]) + ' ')\n",
      "del H_coords_arr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`Y`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npzfile = np.load(\"../working/Y.npz\")\n",
      "Y = scipy.sparse.csr_matrix((npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']))\n",
      "Y_nonzeros = Y.nonzero()\n",
      "del Y\n",
      "Y_coords_arr = np.vstack(Y_nonzeros)\n",
      "del Y_nonzeros\n",
      "np.save(\"../working/Y_coords_arr.npy\", Y_coords_arr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if 'Y_coords_arr' not in globals():\n",
      "    Y_coords_arr = np.load(\"../working/Y_coords_arr.npy\")\n",
      "    \n",
      "with open(\"../working/Y.txt\", 'wb') as outfile:\n",
      "    curr_i = -1\n",
      "    for x in np.nditer(Y_coords_arr, flags=['external_loop'], order='F'):\n",
      "        if x[0] > curr_i:\n",
      "            curr_i = x[0]\n",
      "            outfile.write('\\n') # quirk: remember to skip the first line...\n",
      "            outfile.write(str(x[0]) + ' ' + str(x[1]) + ' ')\n",
      "        else:\n",
      "            outfile.write(str(x[1]) + ' ')\n",
      "del Y_coords_arr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let `C` equal `H == Y`, and `D` is conceptually the negative of `C`, (`H != Y`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Helper code\n",
      "import collections\n",
      "\n",
      "class OrderedSet(collections.MutableSet):\n",
      "\n",
      "    def __init__(self, iterable=None):\n",
      "        self.end = end = [] \n",
      "        end += [None, end, end]         # sentinel node for doubly linked list\n",
      "        self.map = {}                   # key --> [key, prev, next]\n",
      "        if iterable is not None:\n",
      "            self |= iterable\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.map)\n",
      "\n",
      "    def __contains__(self, key):\n",
      "        return key in self.map\n",
      "\n",
      "    def add(self, key):\n",
      "        if key not in self.map:\n",
      "            end = self.end\n",
      "            curr = end[1]\n",
      "            curr[2] = end[1] = self.map[key] = [key, curr, end]\n",
      "\n",
      "    def discard(self, key):\n",
      "        if key in self.map:        \n",
      "            key, prev, next = self.map.pop(key)\n",
      "            prev[2] = next\n",
      "            next[1] = prev\n",
      "\n",
      "    def __iter__(self):\n",
      "        end = self.end\n",
      "        curr = end[2]\n",
      "        while curr is not end:\n",
      "            yield curr[0]\n",
      "            curr = curr[2]\n",
      "\n",
      "    def __reversed__(self):\n",
      "        end = self.end\n",
      "        curr = end[1]\n",
      "        while curr is not end:\n",
      "            yield curr[0]\n",
      "            curr = curr[1]\n",
      "\n",
      "    def pop(self, last=True):\n",
      "        if not self:\n",
      "            raise KeyError('set is empty')\n",
      "        key = self.end[1][0] if last else self.end[2][0]\n",
      "        self.discard(key)\n",
      "        return key\n",
      "\n",
      "    def __repr__(self):\n",
      "        if not self:\n",
      "            return '%s()' % (self.__class__.__name__,)\n",
      "        return '%s(%r)' % (self.__class__.__name__, list(self))\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        if isinstance(other, OrderedSet):\n",
      "            return len(self) == len(other) and list(self) == list(other)\n",
      "        return set(self) == set(other)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H_file = open(\"../working/H.txt\", 'rb')\n",
      "Y_file = open(\"../working/Y.txt\", 'rb')\n",
      "C_file = open(\"../working/C.txt\", 'wb')\n",
      "\n",
      "tp = 0\n",
      "with H_file, Y_file, C_file:\n",
      "    flag1, flag2, flag3 = True, True, True\n",
      "    # Skip the 1st lines\n",
      "    Y_splits = Y_file.readline().split()\n",
      "    H_splits = H_file.readline().split()\n",
      "    # Default dummy values\n",
      "    curr_row, H_row = -1, -1\n",
      "    while flag1:\n",
      "        if flag2:\n",
      "            Y_line = Y_file.readline()\n",
      "            Y_splits = Y_line.split()\n",
      "        if flag3:\n",
      "            H_line = H_file.readline()\n",
      "            H_splits = H_line.split()\n",
      "        \n",
      "        # Y should be row-indexed...\n",
      "        try:\n",
      "            curr_row = Y_splits[0]\n",
      "            H_row = H_splits[0]\n",
      "        except:\n",
      "            flag1 = False\n",
      "            continue\n",
      "        \n",
      "        if curr_row == H_row:\n",
      "            intersected = OrderedSet(Y_splits[1:]) & OrderedSet(H_splits[1:])\n",
      "            intersected_len = len(intersected)\n",
      "            if intersected_len != 0:\n",
      "                tp += intersected_len\n",
      "                C_file.write(curr_row + ' ' +\\\n",
      "                             ' '.join((ind for ind in intersected)) + '\\n')\n",
      "            flag2, flag3 = True, True\n",
      "        elif curr_row < H_row:\n",
      "            flag2 = True\n",
      "            flag3 = False\n",
      "        else:\n",
      "            flag2 = False\n",
      "            flag3 = True\n",
      "print 'tp:', tp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tp: 8198\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`(H != Y) & (H == 1)`, or just `(H != Y) & H`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "with open(\"../working/tagsdict.pickle\", 'rb') as picklefile:\n",
      "    tagsdict = pickle.load(picklefile)\n",
      "\n",
      "n_unique_tags = int(tagsdict.__str__().lstrip('Dictionary(').rstrip(\n",
      "    ' unique tokens)'))\n",
      "\n",
      "del tagsdict\n",
      "print n_unique_tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20610\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H_file = open(\"../working/H.txt\", 'rb')\n",
      "C_file = open(\"../working/C.txt\", 'rb')\n",
      "\n",
      "fp = 0\n",
      "tn = 0\n",
      "with C_file, H_file:\n",
      "    # Skip the first line, of course\n",
      "    H_line = H_file.readline()\n",
      "    C_line = C_file.readline()\n",
      "    flag1, flag2, flag3 = True, True, True\n",
      "    \n",
      "    while flag1:\n",
      "        if flag2:\n",
      "            H_line = H_file.readline()\n",
      "            H_splits = H_line.split()\n",
      "        if flag3:\n",
      "            C_line = C_file.readline()\n",
      "            C_splits = C_line.split()\n",
      "        try:\n",
      "            H_row = H_splits[0]\n",
      "            C_row = C_splits[0]\n",
      "        except:\n",
      "            flag1 = False\n",
      "            continue\n",
      "        \n",
      "        if H_row == C_row:\n",
      "            H_indices_set = set(H_splits[1:])\n",
      "            C_indices_set = set(C_splits[1:])\n",
      "            # calculate the false-positive rate\n",
      "            fp_differed = H_indices_set - C_indices_set\n",
      "            fp_differed_len = len(fp_differed)\n",
      "            if fp_differed_len != 0:\n",
      "                fp += fp_differed_len\n",
      "            # calculate the true-negative rate\n",
      "            tn_differed = C_indices_set - H_indices_set\n",
      "            tn_differed_len = len(tn_differed)\n",
      "            if tn_differed_len != 0:\n",
      "                tn += tn_differed_len\n",
      "        elif H_row < C_row:\n",
      "            flag2 = True\n",
      "            flag3 = False\n",
      "        else:\n",
      "            flag2 = False\n",
      "            flag3 = True\n",
      "print 'fp:', fp\n",
      "print 'tn:', tn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fp: 51520\n",
        "tn: 0\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npzfile = np.load(\"../working/H.npz\")\n",
      "H = scipy.sparse.csr_matrix((npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']))\n",
      "print H.shape\n",
      "del H"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(364379, 20610)\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tp = 8198\n",
      "fp = 51520\n",
      "tn = 0\n",
      "total_possible = 364379 * 20610\n",
      "fn = total_possible - tp - fp - tn\n",
      "\n",
      "p = 1.0 * tp / (tp + fp)\n",
      "r = 1.0 * tp / (tp + fn)\n",
      "F1 = 2 * (p * r) / (p + r)\n",
      "\n",
      "print \"true positive = %d\" % tp\n",
      "print \"false positive = %d\" % fp\n",
      "print \"false negative = %d\" % fn\n",
      "print\n",
      "print \"precision = %.4f\" % p\n",
      "print \"recall = %.4f\" % r\n",
      "print \"F1-score = %.4f\" % F1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "true positive = 8198\n",
        "false positive = 51520\n",
        "false negative = 7509791472\n",
        "\n",
        "precision = 0.1373\n",
        "recall = 0.0000\n",
        "F1-score = 0.0000\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}