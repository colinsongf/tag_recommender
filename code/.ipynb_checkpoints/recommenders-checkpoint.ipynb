{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "client = Client()\n",
      "dv = client[:]\n",
      "for i, target in enumerate(dv.targets):\n",
      "    dv.client[target]['eid'] = i\n",
      "print dv['eid']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Preprocessing\n",
      "##Duplicate Removal"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "\n",
      "def remove_duplicate_rows(infilename, outfilename, key=1):\n",
      "    ''' Remove duplicate rows on the `key`th column from the inputted csv file '''\n",
      "    dupe_set = set()\n",
      "    fin, fout = open(infilename), open(outfilename, 'w')\n",
      "    with fin, fout:\n",
      "        rdr, wtr = csv.reader(fin, delimiter=','), csv.writer(fout, delimiter=',')\n",
      "        headers = rdr.next() # Ignore headers\n",
      "        for row in rdr:\n",
      "            if row[key] not in dupe_set:\n",
      "                dupe_set.add(row[key])\n",
      "                wtr.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##NLTK Processing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, csv, re, pickle\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Treebank-POS tags stop-list\n",
      "REJECT = set([\"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"LS\", \"MD\", \"PDT\", \"POS\",\n",
      "              \"PRP\", \"PRP$\", \"RB\", \"RP\", \"SYM\", \"TO\", \"UH\", \"WDT\", \"WP\",\n",
      "              \"WP$\", \"WRB\"])\n",
      "              \n",
      "ACCEPT = set([\"FW\", \"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNP\", \"NNPS\", \"NNS\",\n",
      "              \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\",\n",
      "              \"VBZ\"])\n",
      "\n",
      "# Manual stop-list\n",
      "sw = set()\n",
      "with open(\"../working/english.stop\") as f:\n",
      "    for l in f:\n",
      "        sw.add(l.rstrip())\n",
      "\n",
      "# NLTK Tokenizer, Tagger\n",
      "tokenizer = RegexpTokenizer(r\"\\b\\w[\\w#+'-]*(?<!\\.$)\")\n",
      "tagger = pickle.load(open(\"../working/treebank_brill_aubt.pickle\"))\n",
      "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
      "\n",
      "# NLTK Lemmatizer\n",
      "from nltk.corpus.reader.wordnet import ADJ, NOUN, ADV, VERB\n",
      "wntag_dict = {'J':ADJ, 'V':VERB, 'N':NOUN, 'R':ADV, '-':NOUN}\n",
      "def tb2wn(x):\n",
      "    ''' Map Treebank POS tag -> WordNet Lemma tag '''\n",
      "    return wntag_dict.get(x[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import islice\n",
      "\n",
      "def process_title(title):\n",
      "    ''' Extract the title string and wrap filter_tokens '''\n",
      "    tokens = tokenizer.tokenize(title)\n",
      "    return filter_tokens(tokens)\n",
      "\n",
      "def process_body(body):\n",
      "    ''' Extract the body string and wrap filter_tokens '''\n",
      "    # Remove code and html-tags\n",
      "    soup = BeautifulSoup(body)\n",
      "    for tag in soup.find_all('p'):\n",
      "        for child in tag.children:\n",
      "            if child.name != 'a':\n",
      "                if child.name == 'code':\n",
      "                    for cc in child.children:\n",
      "                        body_lst.append(cc.__repr__()[2:-1])\n",
      "                else:\n",
      "                    body_lst.append(child.__repr__()[2:-1])\n",
      "    \n",
      "    # Extract tokens from processed body\n",
      "    tokens_lst = tokenizer.batch_tokenize(body_lst)\n",
      "    tokens = []\n",
      "    for tl in tokens_lst:\n",
      "        tokens.extend(tl)\n",
      "    return filter_tokens(tokens)\n",
      "\n",
      "def filter_tokens(tokens):\n",
      "    ''' Given a list of tokens, remove token if in REJECT or sw\n",
      "        Return as concatenated string '''\n",
      "    tkn_tag_tuples = [(token.lower(), tb2wn(tag)) for (token,tag) in tagger.tag(tokens)\n",
      "                          if tag not in REJECT]\n",
      "    lemmaed_tokens = [lemmatizer.lemmatize(*tup) for tup in tkn_tag_tuples\n",
      "                          if (tup[1] is not None) and (tup[0] not in sw)]\n",
      "    return ' '.join(lemmaed_tokens).decode('ascii', errors='ignore')\n",
      "\n",
      "def filter_html_tags(body):\n",
      "    ''' Use BeautifulSoup to remove html tags '''\n",
      "    pass\n",
      "\n",
      "def process_csv(infilename, outfilename, start=None, stop=None):\n",
      "    with open(outfilename, 'w') as f:\n",
      "        wtr = csv.writer(f, delimiter=',')\n",
      "        for row in row_stream(infilename, start=start, stop=stop):\n",
      "            ID, title, body, tags = row\n",
      "            wtr.writerow([ ID, process_title(title.decode('ascii', errors='ignore')),\n",
      "                           process_body(body.decode('ascii', errors='ignore')), tags ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def row_stream(filename, start=None, stop=None):\n",
      "    ''' A csv-row generator, starting on row `start` and ending on row `stop` '''\n",
      "    with open(filename) as f:\n",
      "        for row in islice(csv.reader(f, delimiter=',', quotechar='\"'), start, stop):\n",
      "            yield row\n",
      "\n",
      "def split_file(infilename, outfiletemplate, n):\n",
      "    ''' Split every n lines into a total of n outfiles. '''\n",
      "    f_handlers = [open(outfiletemplate % eid, 'w') for eid in xrange(n)]\n",
      "    with open(infilename) as f:\n",
      "        for ln,line in enumerate(f):\n",
      "            f_handlers[ln % n].write(line)\n",
      "\n",
      "def split_csv(infilename, outfiletemplate, n):\n",
      "    ''' Split every n lines into a total of n outfiles. '''\n",
      "    f_handlers = [csv.writer(open(outfiletemplate % eid, 'w'), delimiter=',')\n",
      "                      for eid in xrange(n)]\n",
      "    with open(infilename) as f:\n",
      "        for rn,row in enumerate(csv.reader(f, delimiter=',', quotechar='\"')):\n",
      "            f_handlers[rn % n].writerow(row)\n",
      "\n",
      "def combine_files(infiletemplate, outfilename, n):\n",
      "    ''' Given the value of n, combine n files into 1.\n",
      "        Merges serially (file after file). '''\n",
      "    with open(outfilename, 'w') as fout:\n",
      "        for eid in xrange(n):\n",
      "            fin = open(infiletemplate % eid)\n",
      "            fout.write(fin.read())\n",
      "            fin.close()\n",
      "\n",
      "def combine_csvs(infiletemplate, outfilename, n):\n",
      "    ''' Given the value of n, combine n csv files into 1.\n",
      "        Merges serially (file after file). '''\n",
      "    with open(outfilename, 'w') as fout:\n",
      "        wtr = csv.writer(fout, delimiter=',')\n",
      "        for eid in xrange(n):\n",
      "            for row in row_stream(infiletemplate % eid):\n",
      "                wtr.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Term Frequency Pruning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.corpora.dictionary import Dictionary\n",
      "import pickle, csv\n",
      "\n",
      "def build_dictionary_from_splits(splits_template, column, n, save_pickle=None):\n",
      "    ''' Build dictionary from splits. If `save_pickle` is provided, then save. '''\n",
      "    unfiltered_dict = Dictionary()\n",
      "    for eid in xrange(n):\n",
      "        unfiltered_dict.add_documents(csv_isolator(\"../data/proc_Train_%d.csv\" % eid, column))\n",
      "    print \"Before filtering,\", unfiltered_dict\n",
      "    if save_pickle:\n",
      "        print \"\\nsaving...\"\n",
      "        unfiltered_dict.save(save_pickle)\n",
      "    \n",
      "    return unfiltered_dict\n",
      "\n",
      "\n",
      "def build_dictionaries_from_splits(splits_template, n, save_pickle_tup=None):\n",
      "    ''' Builds all 3 dictionaries from splits. If provided, `save_pickle_tup` must\n",
      "        be a 3-tuple of the picklefile names in the following order:\n",
      "        \n",
      "        (title, body, tags)\n",
      "        \n",
      "        If `save_pickle_tup[i]` is None, the corresponding dictionary will not be saved.\n",
      "    '''\n",
      "    utitledict, ubodydict, utagdict = Dictionary(), Dictionary(), Dictionary()\n",
      "    for eid in xrange(n):\n",
      "        for row in row_stream(splits_template % eid):\n",
      "            ID, title, body, tags = row\n",
      "            utitledict.doc2bow(title.split(), allow_update=True)\n",
      "            ubodydict.doc2bow(body.split(), allow_update=True)\n",
      "            utagdict.doc2bow(tags.split(), allow_update=True)\n",
      "    \n",
      "    assert ubodydict.num_docs == utitledict.num_docs == utagdict.num_docs\n",
      "    print \"Before filtering...\"\n",
      "    print \"utitledict:\", utitledict\n",
      "    print \"ubodydict:\", ubodydict\n",
      "    print \"utagdict:\", utagdict\n",
      "    \n",
      "    if save_pickle_tup:\n",
      "        assert len(save_pickle_tup) == 3\n",
      "        if save_pickle_tup[0]:\n",
      "            print \"saving utitledict...\"\n",
      "            utitledict.save(save_pickle_tup[0])\n",
      "        if save_pickle_tup[1]:\n",
      "            print \"saving ubodydict...\"\n",
      "            ubodydict.save(save_pickle_tup[1])\n",
      "        if save_pickle_tup[2]:\n",
      "            print \"saving utagdict...\"\n",
      "            utagdict.save(save_pickle_tup[2])\n",
      "            \n",
      "    return (utitledict, ubodydict, utagdict)\n",
      "\n",
      "\n",
      "def filter_extremes_wrapper(gdict, no_below=1, no_above=1.0, keep_n=None, save_pickle=None):\n",
      "    ''' Given unfiltered gensim-dict `gdict`, wrap filter_extremes '''\n",
      "    if type(gdict) == str:\n",
      "        gdict = Dictionary.load(gdict)\n",
      "    print \"Before filtering:\", gdict\n",
      "    gdict.filter_extremes(**kwargs)\n",
      "    print \"After filtering:\", gdict\n",
      "    \n",
      "    if save_pickle:\n",
      "        print \"\\nsaving...\"\n",
      "        gdict.save(save_pickle)\n",
      "    \n",
      "    return gdict\n",
      "\n",
      "\n",
      "def csv_isolator(filename, column):\n",
      "    ''' Isolate a column in the csv-file and yield a list containing its contents. '''\n",
      "    for row in row_stream(filename):\n",
      "        yield row[column].split()\n",
      "\n",
      "\n",
      "def prune_csv_file1(infilename, outfilename, column, gensim_dict):\n",
      "    ''' Using the (one) provided gensim.corpora.dictionary.Dictionary, prune out\n",
      "        tokens not found in the filtered dictionary. If filtered examples\n",
      "        have no tokens, remove them from file. '''\n",
      "    with open(outfilename, 'w') as f:\n",
      "        wtr = csv.writer(f, delimiter=',')\n",
      "        for row in row_stream(infilename):\n",
      "            tokens = row[column].split()\n",
      "            filtered_tokens = [token for token in tokens if token in gensim_dict.token2id]\n",
      "            if not filtered_tokens: # if no tokens remain, remove\n",
      "                continue\n",
      "            row[column] = ' '.join(filtered_tokens) # Python generators are un-affected from this\n",
      "            wtr.writerow(row)\n",
      "\n",
      "\n",
      "def prune_csv_file2(infilename, outfilename, gdict_tup, col_tup):\n",
      "    ''' `gdict_tup` must be a 2-tuple of gensim-dicts.\n",
      "        `col_tup` are the matching selection columns.\n",
      "        Prune a csv-file with 2 dictionaries simultaneously. '''\n",
      "    col_a, col_b = col_tup\n",
      "    gdict_a, gdict_b = gdict_tup\n",
      "    with open(outfilename, 'w') as f:\n",
      "        wtr = csv.writer(f, delimiter=',')\n",
      "        for row in row_stream(infilename):\n",
      "            tokens_a, tokens_b = row[col_a].split(), row[col_b].split()\n",
      "            filtered_tokens_a = [token for token in tokens_a if token in gdict_a.token2id]\n",
      "            filtered_tokens_b = [token for token in tokens_b if token in gdict_b.token2id]\n",
      "            if not filtered_tokens_a or not filtered_tokens_b:\n",
      "                continue\n",
      "            row[col_a] = ' '.join(filtered_tokens_a)\n",
      "            row[col_b] = ' '.join(filtered_tokens_b)\n",
      "            wtr.writerow(row)\n",
      "\n",
      "\n",
      "def prune_csv_file3(infilename, outfilename, gdict_tup, col_tup):\n",
      "    ''' `gdict_tup` must be a 3-tup\n",
      "        `col_tup` are the matching selection columns.\n",
      "        Prune a csv-file with all 3 dictionaries simultaneously. '''\n",
      "    col_a, col_b, col_c = col_tup\n",
      "    gdict_a, gdict_b, gdict_c = gdict_tup\n",
      "    with open(outfilename, 'w') as f:\n",
      "        wtr = csv.writer(f, delimiter=',')\n",
      "        for row in row_stream(infilename):\n",
      "            tokens_a, tokens_b, tokens_c = row[col_a].split(), row[col_b].split(), row[col_c].split()\n",
      "            filtered_tokens_a = [token for token in tokens_a if token in gdict_a.token2id]\n",
      "            filtered_tokens_b = [token for token in tokens_b if token in gdict_b.token2id]\n",
      "            filtered_tokens_c = [token for token in tokens_c if token in gdict_c.token2id]\n",
      "            if not filtered_tokens_a or not filtered_tokens_b or not filtered_tokens_c:\n",
      "                continue\n",
      "            row[col_a] = ' '.join(filtered_tokens_a)\n",
      "            row[col_b] = ' '.join(filtered_tokens_b)\n",
      "            row[col_c] = ' '.join(filtered_tokens_c)\n",
      "            wtr.writerow(row)    \n",
      "\n",
      "\n",
      "def extract_csv(infilename, outfilename, column):\n",
      "    ''' Isolate a column in the csv-file and write it into another file. '''\n",
      "    with open(outfilename, 'w') as f:\n",
      "        for row in row_stream(infilename):\n",
      "            f.write(row[column] + '\\n')\n",
      "            \n",
      "def extract_from_csvs(infiletemplate, outfilename, n, column):\n",
      "    ''' Isolate a column from multiple csv-files and write them\n",
      "        serially into another file. '''\n",
      "    with open(outfilename, 'w') as f:\n",
      "        for eid in xrange(n):\n",
      "            for row in row_stream(infiletemplate % eid):\n",
      "                f.write(row[column] + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import autojit\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from __future__ import division\n",
      "import operator\n",
      "\n",
      "@autojit\n",
      "def delta(a, b):\n",
      "    ''' Record the delta changes from `a` into `b`. '''\n",
      "    b[0] = a[0]\n",
      "    for i in xrange(1, len(a)):\n",
      "        b[i] = b[i-1] + a[i]\n",
      "\n",
      "def plot_dict_hist(gdict):\n",
      "    ''' Provided gensim-dict `gdict`, plot hist statistics '''\n",
      "    if type(gdict) == str:\n",
      "        gdict = Dictionary.load(gdict)\n",
      "    sorted_dfs = sorted(gdict.dfs.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
      "    y = [tup[1] for tup in sorted_dfs]\n",
      "    x = arange(0, len(y))\n",
      "    \n",
      "    plt.figure(figsize=(8,5));\n",
      "    plt.loglog(x, y);\n",
      "    plt.grid();\n",
      "    plt.xlabel(\"Token rank\");\n",
      "    plt.ylabel(\"Document count\");\n",
      "    \n",
      "    cdf = np.empty(len(y))\n",
      "    delta(y, cdf)\n",
      "    cdf /= np.max(cdf) # normalize\n",
      "    \n",
      "    x50 = x[cdf > 0.50][0]\n",
      "    x80 = x[cdf > 0.80][0]\n",
      "    x90 = x[cdf > 0.90][0]\n",
      "    x95 = x[cdf > 0.95][0]\n",
      "    \n",
      "    plt.axvline(x50, color='c');\n",
      "    plt.axvline(x80, color='g');\n",
      "    plt.axvline(x90, color='r');\n",
      "    plt.axvline(x95, color='k');\n",
      "    \n",
      "    print \"50%\\t\", x50\n",
      "    print \"80%\\t\", x80\n",
      "    print \"90%\\t\", x90\n",
      "    print \"95%\\t\", x95\n",
      "\n",
      "def analyze_top_dfs(tokendict, tagdict, cutoff_factor=1):\n",
      "    ''' Provided gensim-dicts `tokendict` and `tagsdict`, show the top word frequencies. '''\n",
      "    if type(tokendict) == str:\n",
      "        tokendict = Dictionary.load(tokendict)\n",
      "    if type(tagdict) == str:\n",
      "        tagdict = Dictionary.load(tagdict)\n",
      "    \n",
      "    max_tag_df = max(tagdict.dfs.iteritems(), key=operator.itemgetter(1))\n",
      "    sorted_dfs = sorted(tokendict.dfs.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
      "    print \"count threshold: %-15s\\t%d\" % (tagdict[max_tag_df[0]], max_tag_df[1])\n",
      "    print \"----------------------------------------------\"\n",
      "    for tup in sorted_dfs[:100]:\n",
      "        if tup[1] > max_tag_df[1] * cutoff_factor:\n",
      "            print \"%-15s\\t%d\" % (tokendict[tup[0]][:15], tup[1])\n",
      "        else: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Main\n",
      "Remove dupes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "remove_duplicate_rows(\"../data/Train.csv\", \"../data/Train_no_dupes.csv\", key=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK Process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "split_csv(\"../data/Train_no_dupes.csv\", \"../data/Train_%d.csv\", 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "client = Client()\n",
      "dv = client[:]\n",
      "for i, target in enumerate(dv.targets):\n",
      "    dv.client[target]['eid'] = i\n",
      "print dv['eid']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 2, 3]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "process_csv(\"../data/Train_%d.csv\" % eid, \"../data/proc_Train_%d.csv\" % eid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prune by term-frequency"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loaded = True\n",
      "if loaded:\n",
      "    utitledict = Dictionary.load(\"../working/utitledict.pickle\")\n",
      "    ubodydict = Dictionary.load(\"../working/ubodydict.pickle\")\n",
      "    utagdict = Dictionary.load(\"../working/utagdict.pickle\")\n",
      "    print \"utitledict:\", utitledict\n",
      "    print \"ubodydict:\", ubodydict\n",
      "    print \"utagdict:\", utagdict\n",
      "    print \"loaded...\"\n",
      "else:\n",
      "    #utagdict = build_dictionary_from_splits(\"../data/proc_Train_%d.csv\",\n",
      "    #                                        -1, 4, \"../working/utagdict.pickle\")\n",
      "    utitledict, ubodydict, utagdict = build_dictionaries_from_splits(\"../data/proc_Train_%d.csv\",\n",
      "        4, save_pickle_tup=(\"../working/utitledict.pickle\", \"../working/ubodydict.pickle\", \"../working/utagdict.pickle\"))\n",
      "    print \"saved...\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Before filtering...\n",
        "utitledict: Dictionary(347642 unique tokens)\n",
        "ubodydict: Dictionary(2287137 unique tokens)\n",
        "utagdict: Dictionary(41573 unique tokens)\n",
        "saving utitledict...\n",
        "saving ubodydict..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "saving utagdict..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze_top_dfs(utitledict, utagdict)\n",
      "print\n",
      "analyze_top_dfs(ubodydict, utagdict, cutoff_factor=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "count threshold: c#             \t327236\n",
        "----------------------------------------------\n",
        "\n",
        "count threshold: c#             \t327236"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "----------------------------------------------\n",
        "work           \t1114324\n",
        "ode            \t797050\n",
        "problem        \t712237\n",
        "file           \t694302\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"utagdict\"\n",
      "plot_dict_hist(utagdict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "utagdict\n",
        "50%\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "197\n",
        "80%\t1865\n",
        "90%\t4659\n",
        "95%\t8882\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFOCAYAAACBjLQUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFXsREZEw9/8BJJaW9nBvuLkAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x153b6190>"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = utitledict.num_docs\n",
      "with open(\"../working/N.pickle\", 'w') as picklefile:\n",
      "    pickle.dump(N, picklefile)\n",
      "print \"number of documents before pruning:\", N"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of documents before pruning: 4125233\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titledict = filter_extremes_wrapper(utitledict, no_below=3, no_above=1.0, keep_n=None,\n",
      "                save_pickle=\"../working/titledict.pickle\")\n",
      "del utitledict\n",
      "\n",
      "with open(\"../working/N.pickle\", 'r') as picklefile:\n",
      "    N = pickle.load(picklefile)\n",
      "no_above = 2*327236/N\n",
      "bodydict = filter_extremes_wrapper(ubodydict, no_below=3, no_above=no_above, keep_n=None,\n",
      "               save_pickle=\"../working/bodydict.pickle\")\n",
      "del ubodydict\n",
      "\n",
      "tagdict = filter_extremes_wrapper(utagdict, no_below=100, no_above=1.0, keep_n=None,\n",
      "              save_pickle=\"../working/tagdict.pickle\")\n",
      "del utagdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Before filtering: Dictionary(347642 unique tokens)\n",
        "After filtering:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Dictionary(90368 unique tokens)\n",
        "Before filtering: Dictionary(2287137 unique tokens)\n",
        "After filtering:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Dictionary(371689 unique tokens)\n",
        "Before filtering: Dictionary(41573 unique tokens)\n",
        "After filtering: Dictionary(8149 unique tokens)\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "titledict = Dictionary.load(\"../working/titledict.pickle\")\n",
      "tagdict = Dictionary.load(\"../working/tagdict.pickle\")\n",
      "\n",
      "prune_csv_file2(\"../data/proc_Train_%d.csv\" % eid, \"../data/pruned_Train_%d.csv\" % eid,\n",
      "    (titledict, tagdict), (1, -1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extract_from_csvs(\"../data/pruned_Train_%d.csv\", \"../data/titleonly.txt\", 4, 1)\n",
      "extract_from_csvs(\"../data/pruned_Train_%d.csv\", \"../data/tagsonly.txt\", 4, -1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "num_eng = 2\n",
      "\n",
      "\n",
      "with open(\"../data/titleonly.txt\") as f_titles:\n",
      "    titles_lst = f_titles.read().split('\\n')[:-1]\n",
      "    tl = len(titles_lst)\n",
      "with open(\"../working/titles_lst.pickle\", 'wb') as picklefile:\n",
      "    pickle.dump(titles_lst, picklefile, -1)\n",
      "for eid in xrange(num_eng):\n",
      "    with open(\"../working/titles_lst_%d.pickle\" % eid, 'wb') as picklefile:\n",
      "        pickle.dump(titles_lst[eid*tl//2 : (eid+1)*tl//2], picklefile, -1)\n",
      "\n",
      "del titles_lst\n",
      "\n",
      "# with open(\"../data/bodyonly.txt\") as f_bodies:\n",
      "#     bodies_lst = f_bodies.read().split('\\n')[:-1]\n",
      "#     bl = len(bodies_lst)\n",
      "# with open(\"../working/bodies_lst.pickle\", 'wb') as picklefile:\n",
      "#     pickle.dump(bodies_lst, picklefile, -1)\n",
      "# for eid in xrange(num_eng):\n",
      "#     with open(\"../working/bodies_lst_%d.pickle\" % eid, 'wb') as picklefile:\n",
      "#         pickle.dump(bodies_lst[eid*bl//2 : (eid+1)*bl//2], picklefile, -1)\n",
      "\n",
      "# del bodies_lst\n",
      "\n",
      "with open(\"../data/tagsonly.txt\") as f_tags:\n",
      "    tags_lst = f_tags.read().split('\\n')[:-1]\n",
      "    tl = len(tags_lst)\n",
      "with open(\"../working/tags_lst.pickle\", 'wb') as picklefile:\n",
      "    pickle.dump(tags_lst, picklefile, -1)\n",
      "for eid in xrange(num_eng):\n",
      "    with open(\"../working/tags_lst_%d.pickle\" % eid, 'wb') as picklefile:\n",
      "        pickle.dump(tags_lst[eid*tl//2 : (eid+1)*tl//2], picklefile, -1)\n",
      "\n",
      "del tags_lst"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Recommenders\n",
      "##`networkx` version\n",
      "###Basic Recommender"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "from itertools import islice\n",
      "\n",
      "def basic_recommender(title, usefulness):\n",
      "    return Counter({word: usefulness.get(word, 0) for word in title.split()})\n",
      "\n",
      "def line_stream(infilename, start=None, stop=None):\n",
      "    with open(infilename) as f:\n",
      "        for line in islice(f, start, stop):\n",
      "            yield line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "from collections import defaultdict\n",
      "import csv\n",
      "\n",
      "assert 3 / 4 == 0.75\n",
      "\n",
      "common, usefulness = defaultdict(int), defaultdict(int)\n",
      "total = Dictionary.load(\"../working/titledict.pickle\")\n",
      "\n",
      "num_eng = 4\n",
      "for eid in xrange(num_eng):\n",
      "    for row in row_stream(\"../data/pruned_Train_%d.csv\" % eid):\n",
      "        ID, title, body, tags = row\n",
      "        title_tokens = title.split()\n",
      "        tags = set(tags.split())\n",
      "        for token in title_tokens:\n",
      "            if token in tags:\n",
      "                common[token] += 1\n",
      "            \n",
      "for (hash_id, count) in total.dfs.iteritems():\n",
      "    token = total[hash_id]\n",
      "    usefulness[token] = common[token] / count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Tag ==> Tag recommender"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "from itertools import combinations\n",
      "\n",
      "G = nx.Graph()\n",
      "\n",
      "num_eng = 4\n",
      "for eid in xrange(num_eng):\n",
      "    for row in row_stream(\"../data/pruned_Train_%d.csv\" % eid):\n",
      "        ID, title, body, tags = row\n",
      "        title_tokens = title.split()\n",
      "        tags = tags.split()\n",
      "        \n",
      "        # add tags\n",
      "        for tag in tags:\n",
      "            if tag:\n",
      "                if not G.has_node(tag):\n",
      "                    G.add_node(tag)\n",
      "                    G.node[tag]['tag_count'] = 1\n",
      "                else:\n",
      "                    G.node[tag]['tag_count'] += 1\n",
      "        \n",
      "        # add edges\n",
      "        for edge in combinations(tags, 2):\n",
      "            ni, nj = edge\n",
      "            if not G.has_edge(ni, nj):\n",
      "                G.add_edge(ni, nj, weight=1)\n",
      "            else:\n",
      "                G.edge[ni][nj]['weight'] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tag2tag_recommender(tags, n_recs=10):\n",
      "    ''' Given a Counter {tags: scores}, generate and score associated tags. '''\n",
      "    \n",
      "    total_scores = Counter()\n",
      "    \n",
      "    # prune tokens with usefulness == 0\n",
      "    tags -= Counter()\n",
      "    \n",
      "    for tag in tags:\n",
      "        tag_scores = Counter({nj: tags[tag]*G.edge[tag][nj]['weight']/G.node[tag]['tag_count']\n",
      "                              for _, nj in G.edges(tag)})\n",
      "        tag_scores = dict(tag_scores.most_common(n_recs)) # keep best n_recs\n",
      "        #total_scores = combine_tags(total_scores, tag_scores)\n",
      "        combine_tags(total_scores, tag_scores)\n",
      "    \n",
      "    return total_scores\n",
      "\n",
      "def combine_tags(c1, c2):\n",
      "    \"\"\"\n",
      "        Probabilistic union of two tag sets.\n",
      "        c1, c2: Counters with tags as key, probs as the values\n",
      "        \n",
      "        Example:\n",
      "\n",
      "            c1 = Counter({'php': 0.2, 'xml': 0.14})\n",
      "            c2 = Counter({'php': 0.1, 'jquery': 0.1})\n",
      "            combine_tags(c1,c2)\n",
      "    \"\"\"\n",
      "    \n",
      "    for t in c1:\n",
      "        if t in c2:\n",
      "            c1[t] = 1 - (1 - c1[t])*(1 - c2[t])\n",
      "    if c2:\n",
      "        for t in c2:\n",
      "            if t not in c1:\n",
      "                c1[t] = c2[t] # add missing items from c2\n",
      "    \n",
      "    #return Counter(dict(c2, **c1)) # add missing items from c2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Title ==> Tag recommender"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = nx.Graph()\n",
      "\n",
      "num_eng = 4\n",
      "for eid in xrange(num_eng):\n",
      "    for row in row_stream(\"../data/pruned_Train_%d.csv\" % eid):\n",
      "        ID, title, body, tags = row\n",
      "        title = title.split()\n",
      "        tags = tags.split()\n",
      "        \n",
      "        # add tags\n",
      "        for token in title:\n",
      "            if not Y.has_node(token):              # add title token if not done so already\n",
      "                Y.add_node(token, count=1)\n",
      "            elif not Y.node[token].get('count'):   # if title token is a tag recently added into the graph,\n",
      "                Y.node[token]['count'] = 1         # then start counting it\n",
      "                \n",
      "            else:                                  # if title token already added, increase its count\n",
      "                Y.node[token]['count'] += 1\n",
      "        \n",
      "        for tag in tags:\n",
      "            if not Y.has_edge(token, tag):\n",
      "                Y.add_edge(token, tag, weight=1)\n",
      "            else:\n",
      "                Y.edge[token][tag]['weight'] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "def title2tag_recommender(tokens, n_recs=10):\n",
      "    total_scores = Counter()\n",
      "    for token in tokens:\n",
      "        if Y.has_node(token):\n",
      "            tag_scores = Counter({nj: Y.edge[token][nj]['weight'] / Y.node[token]['count']\n",
      "                              for _,nj in Y.edges(token)})\n",
      "            tag_scores = dict(tag_scores.most_common(n_recs))\n",
      "            #total_scores = combine_tags(total_scores, tag_scores)\n",
      "            combine_tags(total_scores, tag_scores)\n",
      "    return total_scores - Counter()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Submission\n",
      "Normalize and combine recommenders"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import izip\n",
      "\n",
      "def normalize(rec, weight=1):\n",
      "    ''' Normalize each recommendation proportionally to the score of the leading tag. '''\n",
      "    if rec: \n",
      "        max_score = rec[max(rec)]\n",
      "        for tag in rec:\n",
      "            rec[tag] = weight * rec[tag] / max_score\n",
      "    return rec\n",
      "\n",
      "def meta_recommender(recommendations, weights):\n",
      "    ''' Probabilistically combine `recommendations` with corresponding `weights` '''\n",
      "    total = Counter()\n",
      "    for rec, weight in izip(recommendations, weights):\n",
      "        rec = normalize(rec, weight)\n",
      "        combine_tags(total, rec) # accumulate tag scores\n",
      "    return total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Selecting tags for submission"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def select_tags(rec, threshold=0.3):\n",
      "    ''' Select at most 5 tags with score greater than `threshold` '''\n",
      "    return [tag for tag, score in rec.most_common(5) if score > threshold]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "tp, fp, fn = 0, 0, 0\n",
      "\n",
      "num_eng = 1\n",
      "start = 20000\n",
      "t0 = time.time()\n",
      "for eid in xrange(num_eng):\n",
      "    for row in row_stream(\"../data/pruned_Train_%d.csv\" % eid, start=start, stop=start+10000):\n",
      "        ID, title, body, tags = row\n",
      "        tags = tags.split()\n",
      "        basic = basic_recommender(title, usefulness)\n",
      "        tag2tag = tag2tag_recommender(basic, 10)\n",
      "        title2tag = title2tag_recommender(basic, 10)\n",
      "        recommendations = meta_recommender([basic - Counter(), tag2tag, title2tag], [0.33, 0.39, 0.33])\n",
      "        selections = select_tags(recommendations, threshold=0.3)\n",
      "        \n",
      "        tags_set = set(tags)\n",
      "        selections_set = set(selections)\n",
      "        tp += len(selections_set & tags_set)\n",
      "        fp += len(selections_set - tags_set)\n",
      "        fn += len(tags_set - selections_set)\n",
      "t1 = time.time()\n",
      "\n",
      "print \"tp:\", tp\n",
      "print \"fp:\", fp\n",
      "print \"fn:\", fn\n",
      "p = tp / (tp + fp)\n",
      "r = tp / (tp + fn)\n",
      "f1 = 2*p*r/(p+r)\n",
      "print \"p:\", p\n",
      "print \"r:\", r\n",
      "print \"f1:\", f1\n",
      "print \"time elapsed:\", t1 - t0, \"seconds\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tp: 7481\n",
        "fp: 42434\n",
        "fn: 19915\n",
        "p: 0.149874787138\n",
        "r: 0.273069061177\n",
        "f1: 0.193530028068\n",
        "time elapsed: 209.28975606 seconds\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##`cython` version\n",
      "Note: `cython` can't re-declare, even if you delete the original variable reference. Also, `pickle` works only with pure-`python` objects. `-.-`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cythonmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython -f\n",
      "from __future__ import division\n",
      "from itertools import islice, combinations, izip\n",
      "import csv, pickle, dill\n",
      "from collections import defaultdict\n",
      "from gensim.corpora.dictionary import Dictionary\n",
      "\n",
      "def row_stream(filename, start=None, stop=None):\n",
      "    ''' A csv-row generator, starting on row `start` and ending on row `stop` '''\n",
      "    with open(filename) as f:\n",
      "        for row in islice(csv.reader(f, delimiter=',', quotechar='\"'), start, stop):\n",
      "            yield row\n",
      "\n",
      "def line_stream(infilename, start=None, stop=None):\n",
      "    with open(infilename) as f:\n",
      "        for line in islice(f, start, stop):\n",
      "            yield line\n",
      "\n",
      "# Determine when building dictionaries, tags_lst &/ titles_lst should be in-core or out-of-core\n",
      "out_of_core_flag = False\n",
      "# Determine whether or not to save dictionaries for external use\n",
      "external_use_flag = True\n",
      "\n",
      "if not out_of_core_flag:\n",
      "    with open(\"../working/tags_lst.pickle\", 'rb') as picklefile:\n",
      "        tags_lst = pickle.load(picklefile)\n",
      "    with open(\"../working/titles_lst.pickle\", 'rb') as picklefile:\n",
      "        titles_lst = pickle.load(picklefile)\n",
      "\n",
      "\n",
      "''' Build usefulness '''\n",
      "common, usefulness = defaultdict(int), defaultdict(int)\n",
      "titledict = Dictionary.load(\"../working/titledict.pickle\")\n",
      "cdef bytes ID, title_str, body, tags_str, token\n",
      "\n",
      "if not out_of_core_flag:\n",
      "    for title_str, tags_str in izip(titles_lst, tags_lst):\n",
      "        tags = tags_str.split()\n",
      "        for token in title_str.split():\n",
      "            if token in tags:\n",
      "                common[token] += 1\n",
      "else:\n",
      "    num_eng = 4\n",
      "    for eid in xrange(num_eng):\n",
      "        for row in row_stream(\"../data/pruned_Train_%d.csv\" % eid):\n",
      "            ID, title_str, body, tags_str = row\n",
      "            tags = tags_str.split()\n",
      "            for token in title_str.split():\n",
      "                if token in tags:\n",
      "                    common[token] += 1\n",
      "\n",
      "cdef int hash_id, count\n",
      "for (hash_id, count) in titledict.dfs.iteritems():\n",
      "    usefulness[titledict[hash_id]] = common[titledict[hash_id]] / count\n",
      "del common, titledict\n",
      "\n",
      "# Save usefulness\n",
      "if external_use_flag:\n",
      "    temp = {}\n",
      "    for t_tag, t_use in usefulness.iteritems():\n",
      "        temp[t_tag] = t_use\n",
      "    with open(\"../working/usefulness.pickle\", 'wb') as picklefile:\n",
      "        pickle.dump(temp, picklefile, -1)\n",
      "    del temp, usefulness\n",
      "\n",
      "\n",
      "''' Build tag_occurrence and tags_co_occurrence '''\n",
      "tag_occurrence, tags_co_occurrence = defaultdict(int), defaultdict(lambda : defaultdict(int))\n",
      "cdef bytes tag, tag2\n",
      "if not out_of_core_flag:\n",
      "    for tags_str in tags_lst:\n",
      "        tags = tags_str.split()\n",
      "        for tag in tags:\n",
      "            tag_occurrence[tag] += 1\n",
      "        for tag, tag2 in combinations(tags, 2):\n",
      "            tags_co_occurrence[tag][tag2] += 1\n",
      "            tags_co_occurrence[tag2][tag] += 1\n",
      "else:\n",
      "    for eid in xrange(num_eng):\n",
      "        for tags_str in line_stream(\"../data/tags/pruned_tagsonly_%d.txt\" % eid):\n",
      "            tags = tags_str.split()\n",
      "            for tag in tags:\n",
      "                tag_occurrence[tag] += 1\n",
      "            for tag, tag2 in combinations(tags, 2):\n",
      "                tags_co_occurrence[tag][tag2] += 1\n",
      "                tags_co_occurrence[tag2][tag] += 1\n",
      "\n",
      "# Save tag_occurrence and tags_co_occurrence\n",
      "if external_use_flag:\n",
      "    temp = {}\n",
      "    for t_tag, t_count in tag_occurrence.iteritems():\n",
      "        temp[t_tag] = t_count\n",
      "    with open(\"../working/tag_occurrence.pickle\", 'wb') as picklefile:\n",
      "        pickle.dump(temp, picklefile, -1)\n",
      "    del temp, tag_occurrence\n",
      "    \n",
      "    temp = {}\n",
      "    for t_token, d in tags_co_occurrence.iteritems():\n",
      "        td = temp[t_token] = {}\n",
      "        for t_tag, t_count in d.iteritems():\n",
      "            td[t_tag] = t_count\n",
      "    with open(\"../working/tags_co_occurrence.pickle\", 'wb') as picklefile:\n",
      "        pickle.dump(temp, picklefile, -1)\n",
      "    del temp, tags_co_occurrence\n",
      "\n",
      "\n",
      "''' Build title_occurrence and title_tag_co_occurrence '''\n",
      "title_occurrence, title_tag_co_occurrence = defaultdict(int), defaultdict(lambda : defaultdict(int))\n",
      "if not out_of_core_flag:\n",
      "    for title_str, tags_str in izip(titles_lst, tags_lst):\n",
      "        for token in title_str.split():\n",
      "            title_occurrence[token] += 1\n",
      "            for tag in tags_str.split():\n",
      "                title_tag_co_occurrence[token][tag] += 1\n",
      "else:\n",
      "    num_eng = 4\n",
      "    for eid in xrange(num_eng):\n",
      "        for row in row_stream(\"../data/pruned_Train_%d.csv\" % eid):\n",
      "            ID, title_str, body, tags_str = row\n",
      "            for token in title_str.split():\n",
      "                title_occurrence[token] += 1\n",
      "                for tag in tags_str.split():\n",
      "                    title_tag_co_occurrence[token][tag] += 1\n",
      "\n",
      "# Save title_occurrence and title_tag_co_occurrence\n",
      "if external_use_flag:\n",
      "    temp = {}\n",
      "    for t_token, t_count in title_occurrence.iteritems():\n",
      "        temp[t_token] = t_count\n",
      "    with open(\"../working/title_occurrence.pickle\", 'wb') as picklefile:\n",
      "        pickle.dump(temp, picklefile, -1)\n",
      "    del temp, title_occurrence\n",
      "    \n",
      "    temp = {}\n",
      "    for t_token, d in title_tag_co_occurrence.iteritems():\n",
      "        td = temp[t_token] = {}\n",
      "        for t_tag, t_count in d.iteritems():\n",
      "            td[t_tag] = t_count\n",
      "    with open(\"../working/title_tag_co_occurrence.pickle\", 'wb') as picklefile:\n",
      "        pickle.dump(temp, picklefile, -1)\n",
      "    del temp, title_tag_co_occurrence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You must manually restart IPython and its engines due to cython's memory leak"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "client = Client()\n",
      "dv = client[:]\n",
      "for i, target in enumerate(dv.targets):\n",
      "    dv.client[target]['eid'] = i\n",
      "print dv['eid']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "%load_ext cythonmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "%%cython -f\n",
      "from __future__ import division\n",
      "from collections import Counter\n",
      "import pickle, csv, time\n",
      "from itertools import izip, islice\n",
      "\n",
      "def row_stream(filename, start=None, stop=None):\n",
      "    ''' A csv-row generator, starting on row `start` and ending on row `stop` '''\n",
      "    with open(filename) as f:\n",
      "        for row in islice(csv.reader(f, delimiter=',', quotechar='\"'), start, stop):\n",
      "            yield row\n",
      "\n",
      "# load from previous state\n",
      "with open(\"../working/usefulness.pickle\", 'rb') as picklefile:\n",
      "    usefulness = pickle.load(picklefile)\n",
      "with open(\"../working/tag_occurrence.pickle\", 'rb') as picklefile:\n",
      "    tag_occurrence = pickle.load(picklefile)\n",
      "with open(\"../working/tags_co_occurrence.pickle\", 'rb') as picklefile:\n",
      "    tags_co_occurrence = pickle.load(picklefile)\n",
      "with open(\"../working/title_occurrence.pickle\", 'rb') as picklefile:\n",
      "    title_occurrence = pickle.load(picklefile)\n",
      "with open(\"../working/title_tag_co_occurrence.pickle\", 'rb') as picklefile:\n",
      "    title_tag_co_occurrence = pickle.load(picklefile)\n",
      "\n",
      "out_of_core_flag = False\n",
      "\n",
      "def basic_recommender(bytes title, usefulness):\n",
      "    return Counter({token: usefulness.get(token, 0) for token in title.split()})\n",
      "\n",
      "def tag2tag_recommender(tags, int n_recs=10):\n",
      "    ''' Given a Counter {tags: scores}, generate and score associated tags. '''\n",
      "    total_scores = Counter()\n",
      "    \n",
      "    # prune tokens with usefulness == 0\n",
      "    tags -= Counter()\n",
      "    \n",
      "    cdef bytes tag, tag2\n",
      "    cdef double use, co_count\n",
      "    for tag,use in tags.iteritems():\n",
      "        tag_scores = Counter({tag2: use*co_count/tag_occurrence[tag]\n",
      "                              for tag2, co_count in tags_co_occurrence[tag].iteritems()})\n",
      "        tag_scores = dict(tag_scores.most_common(n_recs)) # keep best n_recs\n",
      "        combine_tags(total_scores, tag_scores)\n",
      "    \n",
      "    return total_scores\n",
      "\n",
      "def combine_tags(c1, c2):\n",
      "    \"\"\"\n",
      "        Probabilistic union of two tag sets.\n",
      "        c1, c2: Counters with tags as key, probs as the values\n",
      "        \n",
      "        Example:\n",
      "\n",
      "            c1 = Counter({'php': 0.2, 'xml': 0.14})\n",
      "            c2 = Counter({'php': 0.1, 'jquery': 0.1})\n",
      "            combine_tags(c1,c2)\n",
      "    \"\"\"\n",
      "    cdef bytes t\n",
      "    for t in c1:\n",
      "        if t in c2:\n",
      "            c1[t] = 1.0 - (1.0 - c1[t])*(1.0 - c2[t])\n",
      "    \n",
      "    for t in c2:\n",
      "        if t not in c1:\n",
      "            c1[t] = c2[t] # add missing items from c2\n",
      "    \n",
      "def title2tag_recommender(tokens, int n_recs=10):\n",
      "    total_scores = Counter()\n",
      "    cdef bytes token, tag\n",
      "    cdef double co_count\n",
      "    for token in tokens:\n",
      "        tag_scores = Counter({tag: co_count / title_occurrence[token]\n",
      "                              for tag, co_count in title_tag_co_occurrence[token].iteritems()})\n",
      "        tag_scores = dict(tag_scores.most_common(n_recs))\n",
      "        combine_tags(total_scores, tag_scores)\n",
      "    return total_scores - Counter()\n",
      "\n",
      "def normalize(rec, double weight):\n",
      "    ''' Normalize each recommendation proportionally to the score of the leading tag. '''\n",
      "    cdef double max_score\n",
      "    cdef bytes tag\n",
      "    if rec:\n",
      "        max_score = rec[max(rec)]\n",
      "        for tag in rec:\n",
      "            rec[tag] = weight * rec[tag] / max_score\n",
      "    return rec\n",
      "\n",
      "def meta_recommender(recommendations, weights):\n",
      "    ''' Probabilistically combine `recommendations` with corresponding `weights`. '''\n",
      "    total = Counter()\n",
      "    cdef double weight\n",
      "    for rec, weight in izip(recommendations, weights):\n",
      "        rec = normalize(rec, weight)\n",
      "        combine_tags(total, rec) # accumulate tag scores\n",
      "    return total\n",
      "\n",
      "def select_tags(rec, double threshold=0.3):\n",
      "    ''' Select at most 5 tags with score greather than `threshold`. '''\n",
      "    cdef bytes tag\n",
      "    cdef double score\n",
      "    return [tag for tag, score in rec.most_common(5) if score > threshold]\n",
      "\n",
      "\n",
      "cdef int tp, fp, fn\n",
      "tp, fp, fn = 0, 0, 0\n",
      "cdef double p, r, f1\n",
      "\n",
      "num_eng = 1\n",
      "n = 10000\n",
      "start = 0\n",
      "\n",
      "cdef double weight\n",
      "cdef double max_score\n",
      "cdef bytes tag\n",
      "\n",
      "cdef bytes ID, title_str, body, tags_str\n",
      "t0 = time.time()\n",
      "\n",
      "if not out_of_core_flag:\n",
      "    with open(\"../working/titles_lst.pickle\", 'rb') as picklefile:\n",
      "        titles_lst = pickle.load(picklefile)\n",
      "    with open(\"../working/tags_lst.pickle\", 'rb') as picklefile:\n",
      "        tags_lst = pickle.load(picklefile)\n",
      "    for title_str, tags_str in islice(izip(titles_lst, tags_lst), start, start+n):\n",
      "        basic = basic_recommender(title_str, usefulness)\n",
      "        tag2tag = tag2tag_recommender(basic, 10)\n",
      "        title2tag = title2tag_recommender(basic, 10)\n",
      "        recommendations = meta_recommender([basic - Counter(), tag2tag, title2tag],\n",
      "                                           [0.33, 0.39, 0.33])\n",
      "        selections = select_tags(recommendations, threshold=0.4)\n",
      "        \n",
      "        tags_set = set(tags_str.split())\n",
      "        selections_set = set(selections)\n",
      "        tp += len(selections_set & tags_set)\n",
      "        fp += len(selections_set - tags_set)\n",
      "        fn += len(tags_set - selections_set)\n",
      "else:\n",
      "    for eid in xrange(num_eng):\n",
      "        for row in row_stream(\"../data/pruned_Train_%d.csv\" % eid, start=start, stop=start+n):\n",
      "            ID, title_str, body, tags_str = row\n",
      "            basic = basic_recommender(title_str, usefulness)\n",
      "            tag2tag = tag2tag_recommender(basic, 10)\n",
      "            title2tag = title2tag_recommender(basic, 10)\n",
      "            recommendations = meta_recommender([basic - Counter(), tag2tag, title2tag],\n",
      "                                               [0.33, 0.39, 0.33])\n",
      "            selections = select_tags(recommendations, threshold=0.4)\n",
      "            \n",
      "            tags_set = set(tags_str.split())\n",
      "            selections_set = set(selections)\n",
      "            tp += len(selections_set & tags_set)\n",
      "            fp += len(selections_set - tags_set)\n",
      "            fn += len(tags_set - selections_set)\n",
      "\n",
      "t1 = time.time()\n",
      "\n",
      "print \"tp:\", tp\n",
      "print \"fp:\", fp\n",
      "print \"fn:\", fn\n",
      "p = tp / (tp + fp)\n",
      "r = tp / (tp + fn)\n",
      "if p == r == 0:\n",
      "    f1 = 0\n",
      "else:\n",
      "    f1 = 2*p*r/(p+r)\n",
      "print \"p:\", p\n",
      "print \"r:\", r\n",
      "print \"f1:\", f1\n",
      "print \"time elapsed:\", t1 - t0, \"seconds\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:0] \n",
        "tp: 7950\n",
        "fp: 41689\n",
        "fn: 19315\n",
        "p: 0.160156328693\n",
        "r: 0.291582615074\n",
        "f1: 0.206751274316\n",
        "time elapsed: 138.360890865 seconds\n",
        "[stdout:1] \n",
        "tp: 7950\n",
        "fp: 41689\n",
        "fn: 19315\n",
        "p: 0.160156328693\n",
        "r: 0.291582615074\n",
        "f1: 0.206751274316\n",
        "time elapsed: 139.516354084 seconds\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyximport; pyximport.install()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "(None, <pyximport.pyximport.PyxImporter at 0x31209d0>)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "from lib.recommender.new_cython_recommender import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import multiprocessing\n",
      "# import logging\n",
      "# import sys\n",
      "\n",
      "# multiprocessing.log_to_stderr(logging.DEBUG)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# weights, threshold args\n",
      "args = [[0.33, 0.39, 0.33], 0.3]\n",
      "# kwargs\n",
      "kwargs = dict(\n",
      "               out_of_core_flag=True,\n",
      "               num_eng = 2,\n",
      "               start = 0,\n",
      "               n = 50,\n",
      "             )\n",
      "\n",
      "print 50*2/10000 * (139-20) + 40"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41.19\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time mp_boiler(*args, **kwargs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "manage_dicts call time elapsed: 40.4363429546 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tp: 177\n",
        "fp: 354\n",
        "fn: 223\n",
        "p: 0.333333333333\n",
        "r: 0.4425\n",
        "f1: 0.380236305048\n",
        "CPU times: user 37.6 s, sys: 716 ms, total: 38.3 s\n",
        "Wall time: 2min 38s\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.3802363050483351"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights = [0.33, 0.39, 0.33]\n",
      "basic = Counter({'mime': 0.29154795821462487, 'image': 0.2118145767810748, 'upload': 0.15020918393064167, 'file': 0.061763216528256025, 'check': 0.006809448462929475, 'type': 0.0})\n",
      "tag2tag = Counter({'php': 0.12632635187299612, 'email': 0.07091707091707092, 'java': 0.057378085097799736, 'c#': 0.049675907239108974, 'android': 0.03411865099539857, 'mime-types': 0.03290922898766036, 'javascript': 0.027868012899552985, 'jquery': 0.026519480745750834, 'html': 0.025699563539166026, 'python': 0.02523303251334419, 'file': 0.02226385226798896, 'image': 0.018708817792939113, 'css': 0.015960075814379183, 'http': 0.012514777220659573, 'smtp': 0.012051266953227738, 'file-upload': 0.011670567721123246, 'perl': 0.010660736150932229, 'image-processing': 0.010591186935374473, 'iphone': 0.00824573377321542, 'ajax': 0.007038250071815866, 'ftp': 0.0067509745586805244, 'c': 0.0036794694943050876, 'c++': 0.003470827128829225, 'windows': 0.002652594856133739, 'upload': 0.0023595535305637274, 'linux': 0.0021235981775073547, 'mysql': 0.0004982523265558153, 'sql': 0.00038752958732118963})\n",
      "title2tag = Counter({'php': 0.5307614410057097, 'c#': 0.45773631357992073, 'java': 0.36125001732387774, 'javascript': 0.3495667494946266, 'mime-types': 0.34277620396600567, 'mime': 0.2898961284230406, 'android': 0.28353683154905907, 'jquery': 0.2486413181886601, 'image': 0.23727467524784285, 'file-upload': 0.19754977029096477, 'upload': 0.1472113817121968, 'c++': 0.14160287457236165, 'asp.net': 0.1286634787408819, 'html': 0.1199324826008542, 'email': 0.10576015108593012, 'css': 0.09406364020224549, 'python': 0.08080886492468553, 'iphone': 0.07156545369899202, '.net': 0.06376049104346737, 'file': 0.05860026232469648, 'types': 0.05449079293498685, 'generics': 0.05447289776489325, 'linux': 0.0448354224820913, 'mysql': 0.03720333547145606, 'windows': 0.03501025756825967, 'perl': 0.03493862134088763, 'custom-post-types': 0.026484851738515773})\n",
      "recommendations = meta_recommender([basic, tag2tag, title2tag], weights)\n",
      "print recommendations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({'php': 0.5913, 'mime': 0.450762416110866, 'c#': 0.3943120976418427, 'image': 0.38933951836324143, 'java': 0.36195978033277043, 'mime-types': 0.29306647887521897, 'javascript': 0.2846786846241397, 'email': 0.2702978000970837, 'android': 0.2630521411058522, 'upload': 0.2514795215426755, 'jquery': 0.22380756745175523, 'file': 0.16539617041820998, 'file-upload': 0.15443067778501984, 'html': 0.14799231239870214, 'python': 0.12422930906068108, 'css': 0.10487486331275075, 'c++': 0.09781323513465212, 'asp.net': 0.07999629344595566, 'iphone': 0.06881956236463904, 'perl': 0.05392034349890806, '.net': 0.03964297407225911, 'http': 0.03863614395327567, 'smtp': 0.037205175658709906, 'linux': 0.03424964784451334, 'types': 0.03387955544485798, 'generics': 0.0338684291540715, 'image-processing': 0.032697555526251254, 'windows': 0.029778509414760457, 'mysql': 0.02463375349924324, 'ajax': 0.02172878015798182, 'ftp': 0.020841891171941747, 'custom-post-types': 0.01646691036400322, 'c': 0.011359412201039998, 'check': 0.00770754151916405, 'sql': 0.0011963975592931796, 'type': 0.0})\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "from collections import Counter\n",
      "from itertools import izip\n",
      "\n",
      "\n",
      "def meta_recommender(recommendations, weights):\n",
      "    ''' Probabilistically combine `recommendations` with corresponding `weights`. '''\n",
      "    total = Counter()\n",
      "    for rec, weight in izip(recommendations, weights):\n",
      "        rec = normalize(rec, weight)\n",
      "        combine_tags(total, rec) # accumulate tag scores\n",
      "    return total\n",
      "\n",
      "def normalize(rec, weight):\n",
      "    ''' Normalize each recommendation proportionally to the score of the leading tag. '''\n",
      "    if rec:\n",
      "        max_score = rec.most_common(1)[0][1]\n",
      "        print \"max_score:\", max_score\n",
      "        for tag in rec:\n",
      "            rec[tag] = weight * rec[tag] / max_score\n",
      "    return rec\n",
      "\n",
      "def combine_tags(c1, c2):\n",
      "    \"\"\"\n",
      "        Probabilistic union of two tag sets.\n",
      "        c1, c2: Counters with tags as key, probs as the values\n",
      "        \n",
      "        Example:\n",
      "\n",
      "            c1 = Counter({'php': 0.2, 'xml': 0.14})\n",
      "            c2 = Counter({'php': 0.1, 'jquery': 0.1})\n",
      "            combine_tags(c1,c2)\n",
      "    \"\"\"\n",
      "    for t in c1:\n",
      "        if t in c2:\n",
      "            c1[t] = 1.0 - (1.0 - c1[t])*(1.0 - c2[t])\n",
      "    \n",
      "    for t in c2:\n",
      "        if t not in c1:\n",
      "            c1[t] = c2[t] # add missing items from c2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights = [0.33, 0.39, 0.33]\n",
      "basic = Counter({'mime': 0.29154795821462487, 'image': 0.2118145767810748, 'upload': 0.15020918393064167, 'file': 0.061763216528256025, 'check': 0.006809448462929475, 'type': 0.0})\n",
      "tag2tag = Counter({'php': 0.12632635187299612, 'email': 0.07091707091707092, 'java': 0.057378085097799736, 'c#': 0.049675907239108974, 'android': 0.03411865099539857, 'mime-types': 0.03290922898766036, 'javascript': 0.027868012899552985, 'jquery': 0.026519480745750834, 'html': 0.025699563539166026, 'python': 0.02523303251334419, 'file': 0.02226385226798896, 'image': 0.018708817792939113, 'css': 0.015960075814379183, 'http': 0.012514777220659573, 'smtp': 0.012051266953227738, 'file-upload': 0.011670567721123246, 'perl': 0.010660736150932229, 'image-processing': 0.010591186935374473, 'iphone': 0.00824573377321542, 'ajax': 0.007038250071815866, 'ftp': 0.0067509745586805244, 'c': 0.0036794694943050876, 'c++': 0.003470827128829225, 'windows': 0.002652594856133739, 'upload': 0.0023595535305637274, 'linux': 0.0021235981775073547, 'mysql': 0.0004982523265558153, 'sql': 0.00038752958732118963})\n",
      "title2tag = Counter({'php': 0.5307614410057097, 'c#': 0.45773631357992073, 'java': 0.36125001732387774, 'javascript': 0.3495667494946266, 'mime-types': 0.34277620396600567, 'mime': 0.2898961284230406, 'android': 0.28353683154905907, 'jquery': 0.2486413181886601, 'image': 0.23727467524784285, 'file-upload': 0.19754977029096477, 'upload': 0.1472113817121968, 'c++': 0.14160287457236165, 'asp.net': 0.1286634787408819, 'html': 0.1199324826008542, 'email': 0.10576015108593012, 'css': 0.09406364020224549, 'python': 0.08080886492468553, 'iphone': 0.07156545369899202, '.net': 0.06376049104346737, 'file': 0.05860026232469648, 'types': 0.05449079293498685, 'generics': 0.05447289776489325, 'linux': 0.0448354224820913, 'mysql': 0.03720333547145606, 'windows': 0.03501025756825967, 'perl': 0.03493862134088763, 'custom-post-types': 0.026484851738515773})\n",
      "recommendations = meta_recommender([basic, tag2tag, title2tag], weights)\n",
      "print recommendations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({'php': 0.5913, 'mime': 0.450762416110866, 'c#': 0.3943120976418427, 'image': 0.38933951836324143, 'java': 0.36195978033277043, 'mime-types': 0.29306647887521897, 'javascript': 0.2846786846241397, 'email': 0.2702978000970837, 'android': 0.2630521411058522, 'upload': 0.2514795215426755, 'jquery': 0.22380756745175523, 'file': 0.16539617041820998, 'file-upload': 0.15443067778501984, 'html': 0.14799231239870214, 'python': 0.12422930906068108, 'css': 0.10487486331275075, 'c++': 0.09781323513465212, 'asp.net': 0.07999629344595566, 'iphone': 0.06881956236463904, 'perl': 0.05392034349890806, '.net': 0.03964297407225911, 'http': 0.03863614395327567, 'smtp': 0.037205175658709906, 'linux': 0.03424964784451334, 'types': 0.03387955544485798, 'generics': 0.0338684291540715, 'image-processing': 0.032697555526251254, 'windows': 0.029778509414760457, 'mysql': 0.02463375349924324, 'ajax': 0.02172878015798182, 'ftp': 0.020841891171941747, 'custom-post-types': 0.01646691036400322, 'c': 0.011359412201039998, 'check': 0.00770754151916405, 'sql': 0.0011963975592931796, 'type': 0.0})\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.29154795821462487"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "basic.most_common(1)[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.29154795821462487"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}